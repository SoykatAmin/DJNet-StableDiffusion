{
"cells": [
{
"cell_type": "markdown",
"id": "5537f4c9",
"metadata": {},
"source": [
"# DJNet-StableDiffusion: Kaggle Training Notebook\n",
"\n",
"This notebook provides a complete workflow for training DJNet-StableDiffusion on Kaggle. DJNet is a novel approach that uses transfer learning from Stable Diffusion to generate smooth DJ transitions between songs.\n",
"\n",
"## What This Notebook Does:\n",
"- **Transfer Learning**: Adapts Stable Diffusion UNet for 3-channel spectrogram input\n",
"- **Audio Processing**: Converts audio to mel-spectrograms for diffusion training\n",
"- **DJ Transitions**: Generates smooth transitions between different songs\n",
"- **Kaggle Optimized**: Memory-efficient training for Kaggle's GPU constraints\n",
"\n",
"## Prerequisites:\n",
"- Kaggle notebook with GPU accelerator (P100 or T4 x2 recommended)\n",
"- DJ transition dataset (audio files + JSON metadata)\n",
"- Internet access enabled for downloading pre-trained models\n",
"\n",
"Let's get started! "
]
},
{
"cell_type": "markdown",
"id": "de89f212",
"metadata": {},
"source": [
"## 1. Setup Kaggle Environment\n",
"\n",
"First, let's check our GPU availability and system resources to ensure we have everything needed for training."
]
},
{
"cell_type": "code",
"execution_count": null,
"id": "e43384c9",
"metadata": {},
"outputs": [],
"source": [
"# Check GPU availability and system resources\n",
"import os\n",
"import sys\n",
"import platform\n",
"import subprocess\n",
"\n",
"print(\"üñ•Ô∏è System Information:\")\n",
"print(f\"Python version: {sys.version}\")\n",
"print(f\"Platform: {platform.platform()}\")\n",
"print(f\"Architecture: {platform.architecture()}\")\n",
"\n",
"# Check if we're running in Kaggle\n",
"if os.path.exists('/kaggle'):\n",
" print(\" Running in Kaggle environment\")\n",
" print(f\"Working directory: {os.getcwd()}\")\n",
" \n",
" # Check available space\n",
" disk_usage = subprocess.run(['df', '-h', '/kaggle'], capture_output=True, text=True)\n",
" print(\"\\n Disk Usage:\")\n",
" print(disk_usage.stdout)\n",
"else:\n",
" print(\" Not running in Kaggle environment\")\n",
"\n",
"# Check for GPU\n",
"try:\n",
" result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
" if result.returncode == 0:\n",
" print(\"\\nüéÆ GPU Information:\")\n",
" print(result.stdout)\n",
" else:\n",
" print(\"\\n No GPU detected or nvidia-smi not available\")\n",
"except FileNotFoundError:\n",
" print(\"\\n nvidia-smi not found - no GPU available\")\n",
"\n",
"print(\"\\n Environment Variables:\")\n",
"for key in ['KAGGLE_KERNEL_RUN_TYPE', 'KAGGLE_DATA_PROXY_PROJECT', 'KAGGLE_USER_SECRETS_TOKEN']:\n",
" if key in os.environ:\n",
" print(f\"{key}: {os.environ[key]}\")\n",
" else:\n",
" print(f\"{key}: Not set\")"
]
},
{
"cell_type": "markdown",
"id": "ed468720",
"metadata": {},
"source": [
"## 2. Clone Repository and Install Dependencies\n",
"\n",
"Now let's clone the DJNet-StableDiffusion repository and install all required packages."
]
},
{
"cell_type": "code",
"execution_count": null,
"id": "e771d0c6",
"metadata": {},
"outputs": [],
"source": [
"# Clone the DJNet-StableDiffusion repository\n",
"!git clone https://github.com/SoykatAmin/DJNet-StableDiffusion.git\n",
"%cd DJNet-StableDiffusion\n",
"\n",
"# Verify the repository structure\n",
"!ls -la"
]
},
{
"cell_type": "code",
"execution_count": null,
"id": "f887c43c",
"metadata": {},
"outputs": [],
"source": [
"# Install required packages for DJNet-StableDiffusion\n",
"print(\" Installing core ML packages...\")\n",
"!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
"\n",
"print(\" Installing diffusion and transformer packages...\")\n",
"!pip install -q diffusers transformers accelerate\n",
"\n",
"print(\" Installing audio processing packages...\")\n",
"!pip install -q librosa soundfile torchaudio\n",
"\n",
"print(\" Installing additional ML packages...\")\n",
"!pip install -q wandb matplotlib seaborn tensorboard\n",
"\n",
"print(\" Installing utility packages...\")\n",
"!pip install -q pyyaml datasets huggingface-hub\n",
"\n",
"print(\" All packages installed successfully!\")\n",
"\n",
"# Verify PyTorch and CUDA installation\n",
"import torch\n",
"print(f\"\\n PyTorch version: {torch.__version__}\")\n",
"print(f\" CUDA available: {torch.cuda.is_available()}\")\n",
"if torch.cuda.is_available():\n",
" print(f\" CUDA version: {torch.version.cuda}\")\n",
" print(f\" GPU device: {torch.cuda.get_device_name(0)}\")\n",
" print(f\" GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
]
},
{
"cell_type": "markdown",
"id": "695cd997",
"metadata": {},
"source": [
"## 3. Configure Kaggle-Specific Settings\n",
"\n",
"Let's set up environment variables and directories optimized for Kaggle's file system."
]
},
{
"cell_type": "code",
"execution_count": null,
"id": "6d213cc1",
"metadata": {},
"outputs": [],
"source": [
"# Configure Kaggle-specific environment variables\n",
"import os\n",
"import gc\n",
"\n",
"# Set cache directories for faster model loading\n",
"os.environ['TRANSFORMERS_CACHE'] = '/kaggle/tmp/transformers_cache'\n",
"os.environ['HF_HOME'] = '/kaggle/tmp/hf_cache'\n",
"os.environ['TORCH_HOME'] = '/kaggle/tmp/torch_cache'\n",
"\n",
"# Memory optimization settings\n",
"os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
"os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
"\n",
"print(\" Environment variables configured:\")\n",
"for key in ['TRANSFORMERS_CACHE', 'HF_HOME', 'TORCH_HOME', 'PYTORCH_CUDA_ALLOC_CONF']:\n",
" print(f\" {key}: {os.environ[key]}\")\n",
"\n",
"# Create necessary directories\n",
"directories = [\n",
" '/kaggle/tmp/transformers_cache',\n",
" '/kaggle/tmp/hf_cache', \n",
" '/kaggle/tmp/torch_cache',\n",
" 'checkpoints',\n",
" 'outputs',\n",
" 'logs',\n",
" 'data'\n",
"]\n",
"\n",
"print(\"\\n Creating directories:\")\n",
"for directory in directories:\n",
" os.makedirs(directory, exist_ok=True)\n",
" print(f\" {directory}\")\n",
"\n",
"# Clear GPU cache\n",
"if torch.cuda.is_available():\n",
" torch.cuda.empty_cache()\n",
" gc.collect()\n",
" print(\"\\nüßπ GPU cache cleared\")\n",
"\n",
"print(\"\\n Kaggle environment configured successfully!\")"
]
},
{
"cell_type": "markdown",
"id": "ea9c891c",
"metadata": {},
"source": [
"## 4. Dataset Setup and Validation\n",
"\n",
"Now let's set up the dataset. You have several options depending on how you've uploaded your data to Kaggle."
]
},
{
"cell_type": "code",
"execution_count": null,
"id": "eb062f08",
"metadata": {},
"outputs": [],
"source": [
"# Check for available datasets in Kaggle input\n",
"import json\n",
"from pathlib import Path\n",
"\n",
"print(\" Checking for available datasets...\")\n",
"\n",
"# Check Kaggle input directory\n",
"input_dir = Path(\"/kaggle/input\")\n",
"if input_dir.exists():\n",
" datasets = list(input_dir.iterdir())\n",
" print(f\" Found {len(datasets)} datasets in /kaggle/input/:\")\n",
" for dataset in datasets:\n",
" print(f\" {dataset.name}\")\n",
" \n",
" # Check for audio files and JSON metadata\n",
" audio_files = list(dataset.glob(\"**/*.wav\")) + list(dataset.glob(\"**/*.mp3\"))\n",
" json_files = list(dataset.glob(\"**/*.json\"))\n",
" \n",
" print(f\" Audio files: {len(audio_files)}\")\n",
" print(f\" üìÑ JSON files: {len(json_files)}\")\n",
" \n",
" if audio_files and json_files:\n",
" print(f\" This looks like a valid DJNet dataset!\")\n",
"else:\n",
" print(\" No /kaggle/input directory found\")\n",
"\n",
"# Option A: Use an existing Kaggle dataset\n",
"print(\"\\n Dataset Setup Options:\")\n",
"print(\"Option A: Copy from Kaggle dataset (recommended)\")\n",
"print(\"Option B: Create sample dataset for testing\")\n",
"print(\"Option C: Upload your own data to /kaggle/input/\")\n",
"\n",
"# Let the user choose which option to use\n",
"dataset_option = input(\"Which option would you like to use? (A/B/C): \").upper().strip()\n",
"\n",
"if dataset_option == \"A\" and input_dir.exists() and len(datasets) > 0:\n",
" # Copy from the first available dataset\n",
" source_dataset = datasets[0]\n",
" print(f\" Using dataset: {source_dataset.name}\")\n",
" !cp -r /kaggle/input/{source_dataset.name}/* ./data/\n",
" \n",
"elif dataset_option == \"B\":\n",
" # Create sample dataset for testing\n",
" print(\" Creating sample dataset...\")\n",
" \n",
" # Create sample JSON metadata\n",
" sample_metadata = {\n",
" \"transitions\": [\n",
" {\n",
" \"id\": \"sample_001\",\n",
" \"preceding_track\": \"track_a.wav\",\n",
" \"following_track\": \"track_b.wav\",\n",
" \"transition_audio\": \"transition_001.wav\",\n",
" \"crossfade_duration\": 8.0,\n",
" \"transition_point\": 4.0,\n",
" \"genre_preceding\": \"house\",\n",
" \"genre_following\": \"techno\",\n",
" \"key_preceding\": \"Cm\",\n",
" \"key_following\": \"Am\",\n",
" \"bpm_preceding\": 128,\n",
" \"bpm_following\": 130\n",
" }\n",
" ]\n",
" }\n",
" \n",
" # Save sample metadata\n",
" with open(\"data/sample_metadata.json\", \"w\") as f:\n",
" json.dump(sample_metadata, f, indent=2)\n",
" \n",
" print(\" Sample dataset created! You'll need to add audio files manually.\")\n",
" \n",
"else:\n",
" print(\"‚ÑπÔ∏è Please upload your dataset to Kaggle and add it as input to this notebook\")\n",
"\n",
"# Verify dataset structure\n",
"print(\"\\n Verifying dataset structure...\")\n",
"data_dir = Path(\"data\")\n",
"if data_dir.exists():\n",
" audio_files = list(data_dir.glob(\"**/*.wav\")) + list(data_dir.glob(\"**/*.mp3\"))\n",
" json_files = list(data_dir.glob(\"**/*.json\"))\n",
" \n",
" print(f\" Dataset Summary:\")\n",
" print(f\" Audio files: {len(audio_files)}\")\n",
" print(f\" üìÑ JSON files: {len(json_files)}\")\n",
" \n",
" if json_files:\n",
" # Load and inspect first JSON file\n",
" with open(json_files[0], 'r') as f:\n",
" sample_data = json.load(f)\n",
" \n",
" if 'transitions' in sample_data:\n",
" print(f\" Transitions: {len(sample_data['transitions'])}\")\n",
" print(\" Valid DJNet dataset format detected!\")\n",
" else:\n",
" print(\" JSON format doesn't match expected DJNet structure\")\n",
"else:\n",
" print(\" No data directory found!\")"
]
},
{
"cell_type": "markdown",
"id": "e4e10731",
"metadata": {},
"source": [
"## 5. üéõÔ∏è Model Configuration\n",
"\n",
"Let's create a YAML configuration file optimized for Kaggle's GPU constraints and your specific dataset."
]
},
{
"cell_type": "code",
"execution_count": null,
"id": "c7e36002",
"metadata": {},
"outputs": [],
"source": [
"# Create Kaggle-optimized configuration\n",
"import yaml\n",
"\n",
"# Check GPU memory to adjust batch size\n",
"gpu_memory = 0\n",
"if torch.cuda.is_available():\n",
" gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
" print(f\"üéÆ GPU Memory: {gpu_memory:.1f} GB\")\n",
"\n",
"# Adjust batch size based on GPU memory\n",
"if gpu_memory >= 16:\n",
" batch_size = 4\n",
" gradient_accumulation_steps = 2\n",
"elif gpu_memory >= 12:\n",
" batch_size = 3\n",
" gradient_accumulation_steps = 3\n",
"else:\n",
" batch_size = 2\n",
" gradient_accumulation_steps = 4\n",
"\n",
"effective_batch_size = batch_size * gradient_accumulation_steps\n",
"print(f\" Optimal settings: batch_size={batch_size}, gradient_accumulation={gradient_accumulation_steps}\")\n",
"print(f\" Effective batch size: {effective_batch_size}\")\n",
"\n",
"# Create configuration dictionary\n",
"config = {\n",
" 'model': {\n",
" 'pretrained_model': \"runwayml/stable-diffusion-v1-5\",\n",
" 'freeze_encoder': False,\n",
" 'in_channels': 3,\n",
" 'out_channels': 1\n",
" },\n",
" \n",
" 'data': {\n",
" 'data_dir': \"./data\",\n",
" 'json_files': None,\n",
" 'sample_rate': 16000,\n",
" 'n_fft': 1024,\n",
" 'hop_length': 256,\n",
" 'n_mels': 128,\n",
" 'spectrogram_size': [128, 128],\n",
" 'normalize': True,\n",
" 'augment': False,\n",
" 'cache_spectrograms': True,\n",
" 'train_split': 0.8,\n",
" 'val_split': 0.2\n",
" },\n",
" \n",
" 'training': {\n",
" 'batch_size': batch_size,\n",
" 'val_batch_size': batch_size,\n",
" 'num_epochs': 15, # Reasonable for Kaggle time limits\n",
" 'learning_rate': 1e-4,\n",
" 'weight_decay': 1e-2,\n",
" 'min_lr': 1e-6,\n",
" 'use_scheduler': True,\n",
" \n",
" # Memory optimization\n",
" 'mixed_precision': True,\n",
" 'gradient_accumulation_steps': gradient_accumulation_steps,\n",
" 'max_grad_norm': 1.0,\n",
" \n",
" # Frequent saving for Kaggle\n",
" 'log_every': 25,\n",
" 'save_every': 100,\n",
" 'validate_every': 50,\n",
" 'save_dir': \"./checkpoints\",\n",
" \n",
" # Hardware optimization\n",
" 'device': \"auto\",\n",
" 'num_workers': 2,\n",
" 'pin_memory': True\n",
" },\n",
" \n",
" 'diffusion': {\n",
" 'num_train_timesteps': 1000,\n",
" 'beta_start': 0.0001,\n",
" 'beta_end': 0.02,\n",
" 'beta_schedule': \"linear\",\n",
" 'clip_sample': True\n",
" },\n",
" \n",
" 'loss': {\n",
" 'mse_weight': 1.0,\n",
" 'perceptual_weight': 0.1,\n",
" 'temporal_weight': 0.05\n",
" },\n",
" \n",
" 'wandb': {\n",
" 'project': \"djnet-kaggle\",\n",
" 'run_name': None,\n",
" 'tags': [\"djnet\", \"diffusion\", \"audio\", \"kaggle\"],\n",
" 'notes': \"Kaggle training run for DJ transition generation\"\n",
" },\n",
" \n",
" 'inference': {\n",
" 'num_inference_steps': 25,\n",
" 'guidance_scale': 1.0\n",
" },\n",
" \n",
" 'resume': {\n",
" 'checkpoint_path': None\n",
" },\n",
" \n",
" 'evaluation': {\n",
" 'generate_samples': True,\n",
" 'num_samples': 4,\n",
" 'save_samples': True,\n",
" 'sample_inference_steps': 10\n",
" }\n",
"}\n",
"\n",
"# Ensure config directory exists\n",
"os.makedirs('configs', exist_ok=True)\n",
"\n",
"# Save configuration\n",
"config_path = 'configs/kaggle_config.yaml'\n",
"with open(config_path, 'w') as f:\n",
" yaml.dump(config, f, default_flow_style=False, indent=2)\n",
"\n",
"print(f\" Configuration saved to {config_path}\")\n",
"\n",
"# Display key configuration details\n",
"print(\"\\n Key Configuration Details:\")\n",
"print(f\" Model: {config['model']['pretrained_model']}\")\n",
"print(f\" üî¢ Batch size: {config['training']['batch_size']}\")\n",
"print(f\" Effective batch size: {effective_batch_size}\")\n",
"print(f\" Epochs: {config['training']['num_epochs']}\")\n",
"print(f\" Sample rate: {config['data']['sample_rate']} Hz\")\n",
"print(f\" üìê Spectrogram size: {config['data']['spectrogram_size']}\")\n",
"print(f\" ‚ö° Mixed precision: {config['training']['mixed_precision']}\")\n",
"print(f\" Save every: {config['training']['save_every']} steps\")"
]
},
{
"cell_type": "markdown",
"id": "63a63ffb",
"metadata": {},
"source": [
"## 6. Training Execution\n",
"\n",
"Now let's start the training process! This will initialize the DJNet model with transfer learning from Stable Diffusion and begin training on your dataset."
]
},
{
"cell_type": "code",
"execution_count": null,
"id": "7eb90541",
"metadata": {},
"outputs": [],
"source": [
"# Execute training with Kaggle optimizations\n",
"import sys\n",
"import time\n",
"from datetime import datetime\n",
"\n",
"# Add src directory to Python path\n",
"sys.path.append('src')\n",
"\n",
"# Setup Weights & Biases (optional)\n",
"wandb_available = False\n",
"try:\n",
" import wandb\n",
" \n",
" # Check if WANDB API key is available\n",
" api_key = os.environ.get('WANDB_API_KEY')\n",
" if api_key:\n",
" wandb.login(key=api_key)\n",
" wandb_available = True\n",
" print(\" Weights & Biases configured\")\n",
" else:\n",
" print(\"‚ÑπÔ∏è WANDB_API_KEY not found - training without W&B logging\")\n",
"except ImportError:\n",
" print(\"‚ÑπÔ∏è wandb not available - training without logging\")\n",
"\n",
"# Start training\n",
"print(\" Starting DJNet-StableDiffusion training...\")\n",
"print(\"=\" * 60)\n",
"\n",
"start_time = time.time()\n",
"\n",
"try:\n",
" # Option 1: Use the dedicated Kaggle training script\n",
" print(\" Running Kaggle-optimized training script...\")\n",
" \n",
" # Construct command with appropriate flags\n",
" cmd = f\"python kaggle_train.py --config configs/kaggle_config.yaml\"\n",
" \n",
" if not wandb_available:\n",
" # Training script should handle this internally\n",
" pass\n",
" \n",
" print(f\" Command: {cmd}\")\n",
" print(\" Training starting... (this may take several hours)\")\n",
" \n",
" # Execute training\n",
" !python kaggle_train.py --config configs/kaggle_config.yaml\n",
" \n",
"except KeyboardInterrupt:\n",
" print(\"\\n Training interrupted by user\")\n",
" \n",
"except Exception as e:\n",
" print(f\"\\n Training failed with error: {e}\")\n",
" \n",
" # Try to save any partial progress\n",
" try:\n",
" print(\" Attempting to save emergency checkpoint...\")\n",
" # Emergency checkpoint saving would be handled by the trainer\n",
" except:\n",
" pass\n",
" \n",
" # Show troubleshooting tips\n",
" print(\"\\n Troubleshooting tips:\")\n",
" print(\"1. Check GPU memory usage - try reducing batch_size\")\n",
" print(\"2. Verify dataset format and file paths\")\n",
" print(\"3. Ensure all dependencies are installed correctly\")\n",
" print(\"4. Check available disk space\")\n",
" \n",
"finally:\n",
" # Calculate training time\n",
" end_time = time.time()\n",
" duration = end_time - start_time\n",
" hours = int(duration // 3600)\n",
" minutes = int((duration % 3600) // 60)\n",
" seconds = int(duration % 60)\n",
" \n",
" print(f\"\\n Total execution time: {hours:02d}:{minutes:02d}:{seconds:02d}\")\n",
" \n",
" # Clean up GPU memory\n",
" if torch.cuda.is_available():\n",
" torch.cuda.empty_cache()\n",
" print(\"üßπ GPU cache cleared\")"
]
},
{
"cell_type": "markdown",
"id": "6a7b661f",
"metadata": {},
"source": [
"## 7. Model Inference and Testing\n",
"\n",
"After training, let's test our model by generating some DJ transitions!"
]
},
{
"cell_type": "code",
"execution_count": null,
"id": "f2523186",
"metadata": {},
"outputs": [],
"source": [
"# Load trained model and run inference\n",
"import glob\n",
"from pathlib import Path\n",
"import matplotlib.pyplot as plt\n",
"import numpy as np\n",
"\n",
"# Find the best checkpoint\n",
"checkpoint_dir = Path(\"checkpoints\")\n",
"checkpoint_files = list(checkpoint_dir.glob(\"*.pt\"))\n",
"\n",
"if checkpoint_files:\n",
" # Find the best checkpoint (usually the one with \"best\" in the name)\n",
" best_checkpoint = None\n",
" for ckpt in checkpoint_files:\n",
" if \"best\" in ckpt.name:\n",
" best_checkpoint = ckpt\n",
" break\n",
" \n",
" if best_checkpoint is None:\n",
" # If no \"best\" checkpoint, use the most recent one\n",
" best_checkpoint = max(checkpoint_files, key=lambda x: x.stat().st_mtime)\n",
" \n",
" print(f\" Loading checkpoint: {best_checkpoint}\")\n",
" \n",
" try:\n",
" # Load the model for inference\n",
" from models.djnet_unet import DJNetUNet\n",
" from diffusion.pipeline import DJNetDiffusionPipeline\n",
" from utils.audio_utils import AudioProcessor\n",
" \n",
" # Initialize model\n",
" model = DJNetUNet(\n",
" pretrained_model=config['model']['pretrained_model'],\n",
" in_channels=config['model']['in_channels'],\n",
" out_channels=config['model']['out_channels']\n",
" )\n",
" \n",
" # Load checkpoint\n",
" checkpoint = torch.load(best_checkpoint, map_location='cpu')\n",
" model.load_state_dict(checkpoint['model_state_dict'])\n",
" model.eval()\n",
" \n",
" if torch.cuda.is_available():\n",
" model = model.cuda()\n",
" \n",
" print(\" Model loaded successfully!\")\n",
" \n",
" # Initialize diffusion pipeline\n",
" pipeline = DJNetDiffusionPipeline(\n",
" unet=model,\n",
" scheduler_config={\n",
" 'num_train_timesteps': config['diffusion']['num_train_timesteps'],\n",
" 'beta_start': config['diffusion']['beta_start'],\n",
" 'beta_end': config['diffusion']['beta_end'],\n",
" 'beta_schedule': config['diffusion']['beta_schedule']\n",
" }\n",
" )\n",
" \n",
" # Initialize audio processor\n",
" audio_processor = AudioProcessor(\n",
" sample_rate=config['data']['sample_rate'],\n",
" n_fft=config['data']['n_fft'],\n",
" hop_length=config['data']['hop_length'],\n",
" n_mels=config['data']['n_mels']\n",
" )\n",
" \n",
" print(\" Pipeline initialized!\")\n",
" \n",
" # Test inference with dummy data (if no real data available)\n",
" print(\"\\n Running inference test...\")\n",
" \n",
" # Create dummy spectrograms for testing\n",
" batch_size = 1\n",
" height, width = config['data']['spectrogram_size']\n",
" \n",
" # Dummy preceding and following spectrograms\n",
" preceding_spec = torch.randn(batch_size, 1, height, width)\n",
" following_spec = torch.randn(batch_size, 1, height, width)\n",
" \n",
" if torch.cuda.is_available():\n",
" preceding_spec = preceding_spec.cuda()\n",
" following_spec = following_spec.cuda()\n",
" \n",
" # Generate transition\n",
" with torch.no_grad():\n",
" generated_transition = pipeline(\n",
" preceding_spectrogram=preceding_spec,\n",
" following_spectrogram=following_spec,\n",
" num_inference_steps=config['inference']['num_inference_steps'],\n",
" guidance_scale=config['inference']['guidance_scale']\n",
" )\n",
" \n",
" print(\" Inference test successful!\")\n",
" print(f\" Generated transition shape: {generated_transition.shape}\")\n",
" \n",
" # Visualize results\n",
" fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
" \n",
" # Convert to numpy for visualization\n",
" if torch.cuda.is_available():\n",
" preceding_np = preceding_spec[0, 0].cpu().numpy()\n",
" following_np = following_spec[0, 0].cpu().numpy()\n",
" generated_np = generated_transition[0, 0].cpu().numpy()\n",
" else:\n",
" preceding_np = preceding_spec[0, 0].numpy()\n",
" following_np = following_spec[0, 0].numpy()\n",
" generated_np = generated_transition[0, 0].numpy()\n",
" \n",
" # Plot spectrograms\n",
" axes[0].imshow(preceding_np, aspect='auto', origin='lower')\n",
" axes[0].set_title('Preceding Spectrogram')\n",
" axes[0].set_xlabel('Time')\n",
" axes[0].set_ylabel('Mel Bins')\n",
" \n",
" axes[1].imshow(generated_np, aspect='auto', origin='lower')\n",
" axes[1].set_title('Generated Transition')\n",
" axes[1].set_xlabel('Time')\n",
" axes[1].set_ylabel('Mel Bins')\n",
" \n",
" axes[2].imshow(following_np, aspect='auto', origin='lower')\n",
" axes[2].set_title('Following Spectrogram')\n",
" axes[2].set_xlabel('Time')\n",
" axes[2].set_ylabel('Mel Bins')\n",
" \n",
" plt.tight_layout()\n",
" plt.savefig('outputs/inference_test.png', dpi=150, bbox_inches='tight')\n",
" plt.show()\n",
" \n",
" print(\" Visualization saved to outputs/inference_test.png\")\n",
" \n",
" except Exception as e:\n",
" print(f\" Inference failed: {e}\")\n",
" import traceback\n",
" traceback.print_exc()\n",
" \n",
"else:\n",
" print(\" No checkpoint files found!\")\n",
" print(\" Available files in checkpoints directory:\")\n",
" if checkpoint_dir.exists():\n",
" for file in checkpoint_dir.iterdir():\n",
" print(f\" üìÑ {file.name}\")\n",
" else:\n",
" print(\" Checkpoints directory doesn't exist\")"
]
},
{
"cell_type": "markdown",
"id": "3a49a4ae",
"metadata": {},
"source": [
"## 8. Download Results\n",
"\n",
"Let's prepare and compress your trained models, checkpoints, and outputs for download."
]
},
{
"cell_type": "code",
"execution_count": null,
"id": "6e207c13",
"metadata": {},
"outputs": [],
"source": [
"# Prepare and compress results for download\n",
"import shutil\n",
"import zipfile\n",
"from datetime import datetime\n",
"\n",
"print(\" Preparing results for download...\")\n",
"\n",
"# Create timestamp for unique filenames\n",
"timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
"\n",
"# List of directories to compress\n",
"directories_to_compress = [\n",
" (\"checkpoints\", f\"djnet_checkpoints_{timestamp}.zip\"),\n",
" (\"outputs\", f\"djnet_outputs_{timestamp}.zip\"),\n",
" (\"logs\", f\"djnet_logs_{timestamp}.zip\"),\n",
" (\"configs\", f\"djnet_configs_{timestamp}.zip\")\n",
"]\n",
"\n",
"compressed_files = []\n",
"\n",
"for directory, archive_name in directories_to_compress:\n",
" if os.path.exists(directory) and os.listdir(directory):\n",
" print(f\" Compressing {directory}...\")\n",
" \n",
" # Create zip archive\n",
" with zipfile.ZipFile(archive_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
" for root, dirs, files in os.walk(directory):\n",
" for file in files:\n",
" file_path = os.path.join(root, file)\n",
" arcname = os.path.relpath(file_path, directory)\n",
" zipf.write(file_path, arcname)\n",
" \n",
" file_size = os.path.getsize(archive_name) / (1024 * 1024) # MB\n",
" print(f\" Created {archive_name} ({file_size:.1f} MB)\")\n",
" compressed_files.append((archive_name, file_size))\n",
" else:\n",
" print(f\" {directory} is empty or doesn't exist, skipping...\")\n",
"\n",
"# Create a summary report\n",
"summary_file = f\"training_summary_{timestamp}.txt\"\n",
"with open(summary_file, 'w') as f:\n",
" f.write(\"DJNet-StableDiffusion Training Summary\\n\")\n",
" f.write(\"=\" * 40 + \"\\n\\n\")\n",
" f.write(f\"Training completed on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
" f.write(f\"Configuration used: configs/kaggle_config.yaml\\n\\n\")\n",
" \n",
" f.write(\"Model Configuration:\\n\")\n",
" f.write(f\" - Base model: {config['model']['pretrained_model']}\\n\")\n",
" f.write(f\" - Input channels: {config['model']['in_channels']}\\n\")\n",
" f.write(f\" - Output channels: {config['model']['out_channels']}\\n\\n\")\n",
" \n",
" f.write(\"Training Configuration:\\n\")\n",
" f.write(f\" - Batch size: {config['training']['batch_size']}\\n\")\n",
" f.write(f\" - Effective batch size: {config['training']['batch_size'] * config['training']['gradient_accumulation_steps']}\\n\")\n",
" f.write(f\" - Epochs: {config['training']['num_epochs']}\\n\")\n",
" f.write(f\" - Learning rate: {config['training']['learning_rate']}\\n\")\n",
" f.write(f\" - Mixed precision: {config['training']['mixed_precision']}\\n\\n\")\n",
" \n",
" f.write(\"Data Configuration:\\n\")\n",
" f.write(f\" - Sample rate: {config['data']['sample_rate']} Hz\\n\")\n",
" f.write(f\" - Spectrogram size: {config['data']['spectrogram_size']}\\n\")\n",
" f.write(f\" - N_FFT: {config['data']['n_fft']}\\n\")\n",
" f.write(f\" - Hop length: {config['data']['hop_length']}\\n\\n\")\n",
" \n",
" f.write(\"Generated Files:\\n\")\n",
" for filename, size in compressed_files:\n",
" f.write(f\" - {filename} ({size:.1f} MB)\\n\")\n",
"\n",
"compressed_files.append((summary_file, os.path.getsize(summary_file) / 1024)) # KB\n",
"\n",
"print(f\"\\nüìÑ Created training summary: {summary_file}\")\n",
"\n",
"# Create a final archive with all results\n",
"final_archive = f\"djnet_complete_results_{timestamp}.zip\"\n",
"print(f\"\\n Creating final archive: {final_archive}\")\n",
"\n",
"with zipfile.ZipFile(final_archive, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
" for filename, _ in compressed_files:\n",
" if os.path.exists(filename):\n",
" zipf.write(filename)\n",
"\n",
"final_size = os.path.getsize(final_archive) / (1024 * 1024) # MB\n",
"print(f\" Final archive created: {final_archive} ({final_size:.1f} MB)\")\n",
"\n",
"# Display download links for Kaggle\n",
"print(\"\\nüîó Download Links:\")\n",
"print(\"In Kaggle, you can download these files from the output section:\")\n",
"\n",
"from IPython.display import FileLink, display\n",
"import IPython.display as ipd\n",
"\n",
"for filename, size in compressed_files:\n",
" if os.path.exists(filename):\n",
" print(f\"üìÑ {filename} ({size:.1f} {'MB' if size > 1 else 'KB'})\")\n",
" display(FileLink(filename))\n",
"\n",
"print(f\"\\n Complete Results:\")\n",
"print(f\"üìÑ {final_archive} ({final_size:.1f} MB)\")\n",
"display(FileLink(final_archive))\n",
"\n",
"# Clean up individual archives (keep only the final one)\n",
"cleanup_choice = input(\"\\nClean up individual archives? (y/n): \").lower().strip()\n",
"if cleanup_choice == 'y':\n",
" for filename, _ in compressed_files[:-1]: # Keep summary file\n",
" if filename != summary_file and os.path.exists(filename):\n",
" os.remove(filename)\n",
" print(f\"üóëÔ∏è Removed {filename}\")\n",
"\n",
"print(\"\\n All results prepared for download!\")\n",
"print(\"\\n Next Steps:\")\n",
"print(\"1. Download the compressed files from the output section\")\n",
"print(\"2. Extract the files on your local machine\")\n",
"print(\"3. Use the checkpoints for further inference or fine-tuning\")\n",
"print(\"4. Check the training summary for detailed information\")\n",
"\n",
"# Final GPU cleanup\n",
"if torch.cuda.is_available():\n",
" torch.cuda.empty_cache()\n",
" print(\"\\nüßπ Final GPU cache cleanup completed\")"
]
},
{
"cell_type": "markdown",
"id": "6cb0363a",
"metadata": {},
"source": [
"## Congratulations!\n",
"\n",
"You've successfully completed the DJNet-StableDiffusion training pipeline on Kaggle! \n",
"\n",
"### What You've Accomplished:\n",
"- **Transfer Learning**: Adapted Stable Diffusion for DJ transition generation\n",
"- **Memory Optimization**: Trained efficiently within Kaggle's GPU constraints \n",
"- **Audio Processing**: Converted audio to spectrograms for diffusion training\n",
"- **Model Training**: Completed full training pipeline with checkpointing\n",
"- **Inference Testing**: Validated model performance with sample generations\n",
"- **Results Export**: Prepared all outputs for download and future use\n",
"\n",
"### Next Steps:\n",
"1. **Download your results** using the links above\n",
"2. **Experiment with inference** using different audio pairs\n",
"3. **Fine-tune the model** with your specific music genres\n",
"4. **Scale up training** with larger datasets and longer training times\n",
"5. **Deploy the model** for real-time DJ transition generation\n",
"\n",
"### üìö Resources:\n",
"- **GitHub Repository**: [SoykatAmin/DJNet-StableDiffusion](https://github.com/SoykatAmin/DJNet-StableDiffusion)\n",
"- **Documentation**: Check the README.md for detailed usage instructions\n",
"- **Configuration**: Use `kaggle_config.yaml` as a template for future runs\n",
"\n",
"### ü§ù Community:\n",
"Feel free to share your results and improvements with the community. Consider contributing back to the repository with enhancements or bug fixes!\n",
"\n",
"---\n",
"\n",
"**Happy DJ Mixing! üéõÔ∏è**"
]
}
],
"metadata": {
"language_info": {
"name": "python"
}
},
"nbformat": 4,
"nbformat_minor": 5
}
