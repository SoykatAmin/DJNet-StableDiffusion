{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "effbc1c1",
   "metadata": {},
   "source": [
    "# DJ Transition Generation Training - Kaggle Edition\n",
    "\n",
    "This notebook trains a deep learning model to generate smooth transitions between music tracks using a U-Net architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934b162a",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "Install required packages that may not be available in Kaggle's default environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ec057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tensorboard -q\n",
    "!pip install soundfile -q\n",
    "!pip install librosa -q\n",
    "!pip install scikit-image -q\n",
    "\n",
    "print(\"All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed40a17",
   "metadata": {},
   "source": [
    "## Setup Environment and Import Libraries\n",
    "\n",
    "Import all necessary libraries and configure GPU settings for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668a2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "from IPython.display import display, Audio, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Optimize PyTorch performance\n",
    "torch.backends.cudnn.benchmark = True if torch.cuda.is_available() else False\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🔧 Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU (training will be much slower)\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08dcab3",
   "metadata": {},
   "source": [
    "## Load Configuration\n",
    "\n",
    "Define model hyperparameters and training configuration optimized for Kaggle environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b6bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Parameters - Optimized for Kaggle\n",
    "SAMPLE_RATE = 22050\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "N_MELS = 128\n",
    "SPECTROGRAM_HEIGHT = 128\n",
    "SPECTROGRAM_WIDTH = 512\n",
    "SEGMENT_DURATION = 15.0\n",
    "\n",
    "# Model Configuration\n",
    "IN_CHANNELS = 3  # [source_a, source_b, noise]\n",
    "OUT_CHANNELS = 1  # transition\n",
    "MODEL_DIM = 512\n",
    "\n",
    "# Training Configuration - Optimized for Kaggle GPU\n",
    "BATCH_SIZE = 8  # Increased batch size for better GPU utilization\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 30  # Reduced epochs for Kaggle time limits\n",
    "WARMUP_EPOCHS = 3\n",
    "\n",
    "# Dataset Configuration\n",
    "TRAIN_SAMPLES = 15000  # Will be determined by your uploaded files\n",
    "VAL_SAMPLES = 2000     # Will be determined by your uploaded files\n",
    "\n",
    "print(\" Configuration loaded:\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   Model Dimension: {MODEL_DIM}\")\n",
    "print(f\"   Training Samples: {TRAIN_SAMPLES}\")\n",
    "print(f\"   Validation Samples: {VAL_SAMPLES}\")\n",
    "print(f\"   Spectrogram Size: {SPECTROGRAM_HEIGHT}x{SPECTROGRAM_WIDTH}\")\n",
    "print(f\"   Segment Duration: {SEGMENT_DURATION}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c287012",
   "metadata": {},
   "source": [
    "## Define Model Architecture\n",
    "\n",
    "Implement the ProductionUNet model with encoder-decoder architecture for DJ transition generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e394dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Production U-Net model for generating DJ transitions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=3, out_channels=1, model_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder path\n",
    "        self.enc1 = self._make_encoder_block(in_channels, 64)\n",
    "        self.enc2 = self._make_encoder_block(64, 128) \n",
    "        self.enc3 = self._make_encoder_block(128, 256)\n",
    "        self.enc4 = self._make_encoder_block(256, 512)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(512, model_dim, 3, padding=1),\n",
    "            nn.BatchNorm2d(model_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(model_dim, model_dim, 3, padding=1),\n",
    "            nn.BatchNorm2d(model_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.2)\n",
    "        )\n",
    "        \n",
    "        # Decoder path\n",
    "        self.upconv4 = nn.ConvTranspose2d(model_dim, 512, 2, stride=2)\n",
    "        self.dec4 = self._make_decoder_block(512 + 256, 512)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec3 = self._make_decoder_block(256 + 128, 256)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec2 = self._make_decoder_block(128 + 64, 128)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec1 = self._make_decoder_block(64, 64)\n",
    "        \n",
    "        # Final output layer\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, out_channels, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def _make_encoder_block(self, in_channels, out_channels):\n",
    "        \"\"\"Create encoder block with convolution, normalization, and pooling\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "    \n",
    "    def _make_decoder_block(self, in_channels, out_channels):\n",
    "        \"\"\"Create decoder block with convolution, normalization, and dropout\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the U-Net\"\"\"\n",
    "        # Encoder path with skip connections\n",
    "        e1 = self.enc1(x)      # [B, 64, H/2, W/2]\n",
    "        e2 = self.enc2(e1)     # [B, 128, H/4, W/4]\n",
    "        e3 = self.enc3(e2)     # [B, 256, H/8, W/8]\n",
    "        e4 = self.enc4(e3)     # [B, 512, H/16, W/16]\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(e4)  # [B, model_dim, H/16, W/16]\n",
    "        \n",
    "        # Decoder path with skip connections\n",
    "        d4 = self.upconv4(bottleneck)     # [B, 512, H/8, W/8]\n",
    "        d4 = torch.cat([d4, e3], dim=1)   # [B, 512+256, H/8, W/8]\n",
    "        d4 = self.dec4(d4)                # [B, 512, H/8, W/8]\n",
    "        \n",
    "        d3 = self.upconv3(d4)             # [B, 256, H/4, W/4]\n",
    "        d3 = torch.cat([d3, e2], dim=1)   # [B, 256+128, H/4, W/4]\n",
    "        d3 = self.dec3(d3)                # [B, 256, H/4, W/4]\n",
    "        \n",
    "        d2 = self.upconv2(d3)             # [B, 128, H/2, W/2]\n",
    "        d2 = torch.cat([d2, e1], dim=1)   # [B, 128+64, H/2, W/2]\n",
    "        d2 = self.dec2(d2)                # [B, 128, H/2, W/2]\n",
    "        \n",
    "        d1 = self.upconv1(d2)             # [B, 64, H, W]\n",
    "        d1 = self.dec1(d1)                # [B, 64, H, W]\n",
    "        \n",
    "        # Final output\n",
    "        output = self.final(d1)           # [B, out_channels, H, W]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count the number of trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "# Test model creation\n",
    "test_model = ProductionUNet(\n",
    "    in_channels=IN_CHANNELS,\n",
    "    out_channels=OUT_CHANNELS,\n",
    "    model_dim=MODEL_DIM\n",
    ")\n",
    "\n",
    "print(f\"Model created successfully!\")\n",
    "print(f\"Total parameters: {test_model.count_parameters():,}\")\n",
    "print(f\"Model size: ~{test_model.count_parameters() * 4 / 1024**2:.1f} MB\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(1, IN_CHANNELS, SPECTROGRAM_HEIGHT, SPECTROGRAM_WIDTH)\n",
    "with torch.no_grad():\n",
    "    test_output = test_model(test_input)\n",
    "print(f\"Model forward pass test successful!\")\n",
    "print(f\"   Input shape: {test_input.shape}\")\n",
    "print(f\"   Output shape: {test_output.shape}\")\n",
    "\n",
    "del test_model, test_input, test_output\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf313f3",
   "metadata": {},
   "source": [
    "## Load Your Own Dataset\n",
    "\n",
    "Upload and process your own audio files for training the DJ transition model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc44f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "class AudioTransitionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for training DJ transition models using real audio files\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, audio_dir, spectrogram_size=(128, 512), \n",
    "                 sample_rate=22050, hop_length=512, split='train', val_split=0.2):\n",
    "        self.audio_dir = Path(audio_dir)\n",
    "        self.height, self.width = spectrogram_size\n",
    "        self.sample_rate = sample_rate\n",
    "        self.hop_length = hop_length\n",
    "        self.split = split\n",
    "        \n",
    "        # Find all audio files\n",
    "        audio_extensions = ['*.wav', '*.mp3', '*.flac', '*.m4a']\n",
    "        audio_files = []\n",
    "        for ext in audio_extensions:\n",
    "            audio_files.extend(glob.glob(str(self.audio_dir / f\"**/{ext}\"), recursive=True))\n",
    "        \n",
    "        if not audio_files:\n",
    "            raise ValueError(f\"No audio files found in {audio_dir}\")\n",
    "        \n",
    "        # Split into train/validation\n",
    "        random.seed(42)\n",
    "        random.shuffle(audio_files)\n",
    "        split_idx = int(len(audio_files) * (1 - val_split))\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.audio_files = audio_files[:split_idx]\n",
    "        else:\n",
    "            self.audio_files = audio_files[split_idx:]\n",
    "        \n",
    "        print(f\"Found {len(audio_files)} total audio files\")\n",
    "        print(f\"{split} split: {len(self.audio_files)} files\")\n",
    "        \n",
    "        # Pre-compute segment duration for consistent sizing\n",
    "        self.segment_duration = self.width * self.hop_length / self.sample_rate  # ~15 seconds\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Generate multiple combinations from available files\n",
    "        return len(self.audio_files) * 3  # 3x augmentation\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and process audio files to create training examples\n",
    "        \n",
    "        Returns:\n",
    "            inputs: [3, H, W] tensor containing [source_a, source_b, noise]\n",
    "            target: [1, H, W] tensor containing target transition\n",
    "        \"\"\"\n",
    "        # Select two different audio files\n",
    "        file_idx = idx % len(self.audio_files)\n",
    "        source_a_path = self.audio_files[file_idx]\n",
    "        \n",
    "        # Select different file for source B\n",
    "        source_b_idx = (file_idx + 1 + (idx // len(self.audio_files))) % len(self.audio_files)\n",
    "        source_b_path = self.audio_files[source_b_idx]\n",
    "        \n",
    "        # Load and process audio files\n",
    "        try:\n",
    "            source_a_spec = self._load_audio_segment(source_a_path)\n",
    "            source_b_spec = self._load_audio_segment(source_b_path)\n",
    "            \n",
    "            # Generate noise for the third channel\n",
    "            noise = torch.randn(self.height, self.width) * 0.05\n",
    "            \n",
    "            # Create target transition\n",
    "            target = self._create_transition_target(source_a_spec, source_b_spec)\n",
    "            \n",
    "            # Stack inputs\n",
    "            inputs = torch.stack([source_a_spec, source_b_spec, noise], dim=0)\n",
    "            target = target.unsqueeze(0)  # Add channel dimension\n",
    "            \n",
    "            return inputs, target\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {source_a_path} or {source_b_path}: {e}\")\n",
    "            # Fallback to synthetic data if file loading fails\n",
    "            return self._generate_fallback_data()\n",
    "    \n",
    "    def _load_audio_segment(self, audio_path):\n",
    "        \"\"\"Load and convert audio file to mel-spectrogram\"\"\"\n",
    "        # Load audio\n",
    "        audio, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
    "        \n",
    "        # Extract random segment if audio is longer than needed\n",
    "        segment_samples = int(self.segment_duration * self.sample_rate)\n",
    "        \n",
    "        if len(audio) > segment_samples:\n",
    "            start_idx = random.randint(0, len(audio) - segment_samples)\n",
    "            audio = audio[start_idx:start_idx + segment_samples]\n",
    "        elif len(audio) < segment_samples:\n",
    "            # Pad if too short\n",
    "            audio = np.pad(audio, (0, segment_samples - len(audio)), mode='constant')\n",
    "        \n",
    "        # Convert to mel-spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio,\n",
    "            sr=self.sample_rate,\n",
    "            n_mels=self.height,\n",
    "            hop_length=self.hop_length,\n",
    "            n_fft=2048,\n",
    "            fmin=20,\n",
    "            fmax=8000\n",
    "        )\n",
    "        \n",
    "        # Convert to log scale and normalize\n",
    "        log_mel = np.log(mel_spec + 1e-6)\n",
    "        \n",
    "        # Resize to exact target size if needed\n",
    "        if log_mel.shape[1] != self.width:\n",
    "            from scipy.ndimage import zoom\n",
    "            zoom_factor = self.width / log_mel.shape[1]\n",
    "            log_mel = zoom(log_mel, (1, zoom_factor), order=1)\n",
    "        \n",
    "        # Normalize to [-1, 1]\n",
    "        log_mel = (log_mel - log_mel.mean()) / (log_mel.std() + 1e-6)\n",
    "        log_mel = np.clip(log_mel, -3, 3) / 3  # Soft clipping\n",
    "        \n",
    "        return torch.FloatTensor(log_mel)\n",
    "    \n",
    "    def _create_transition_target(self, source_a, source_b):\n",
    "        \"\"\"Create target transition between two audio spectrograms\"\"\"\n",
    "        transition = torch.zeros_like(source_a)\n",
    "        \n",
    "        # Create smooth crossfade with creative elements\n",
    "        for i in range(self.width):\n",
    "            # Main crossfade curve\n",
    "            alpha = i / self.width\n",
    "            \n",
    "            # Add beat-sync variations\n",
    "            beat_sync = 0.1 * np.sin(2 * np.pi * alpha * 8)  # 8 beats across transition\n",
    "            alpha = np.clip(alpha + beat_sync, 0, 1)\n",
    "            \n",
    "            # Crossfade with frequency-dependent mixing\n",
    "            transition[:, i] = (1 - alpha) * source_a[:, i] + alpha * source_b[:, i]\n",
    "        \n",
    "        # Add transition-specific effects\n",
    "        # Enhance bass frequencies during transition\n",
    "        bass_region = slice(0, self.height // 6)\n",
    "        transition[bass_region, self.width//3:2*self.width//3] *= 1.2\n",
    "        \n",
    "        # Add high-frequency sweep\n",
    "        for i in range(self.width//4, 3*self.width//4):\n",
    "            sweep_freq = int(self.height//2 + (self.height//4) * (i - self.width//4) / (self.width//2))\n",
    "            if sweep_freq < self.height:\n",
    "                transition[sweep_freq, i] += 0.3\n",
    "        \n",
    "        # Normalize\n",
    "        transition = (transition - transition.mean()) / (transition.std() + 1e-6)\n",
    "        transition = np.clip(transition, -3, 3) / 3\n",
    "        \n",
    "        return transition\n",
    "    \n",
    "    def _generate_fallback_data(self):\n",
    "        \"\"\"Generate synthetic data as fallback\"\"\"\n",
    "        print(\"Using synthetic fallback data\")\n",
    "        # Simple synthetic spectrograms\n",
    "        source_a = torch.randn(self.height, self.width) * 0.5\n",
    "        source_b = torch.randn(self.height, self.width) * 0.5\n",
    "        noise = torch.randn(self.height, self.width) * 0.05\n",
    "        \n",
    "        # Simple crossfade target\n",
    "        target = torch.zeros_like(source_a)\n",
    "        for i in range(self.width):\n",
    "            alpha = i / self.width\n",
    "            target[:, i] = (1 - alpha) * source_a[:, i] + alpha * source_b[:, i]\n",
    "        \n",
    "        inputs = torch.stack([source_a, source_b, noise], dim=0)\n",
    "        target = target.unsqueeze(0)\n",
    "        \n",
    "        return inputs, target\n",
    "\n",
    "# Dataset configuration\n",
    "print(\"Setting up audio dataset...\")\n",
    "print(\"Instructions:\")\n",
    "print(\"   1. Upload your audio files to Kaggle (Add Data → Upload)\")\n",
    "print(\"   2. Update AUDIO_DATA_PATH below to point to your uploaded folder\")\n",
    "print(\"   3. Supported formats: WAV, MP3, FLAC, M4A\")\n",
    "print(\"   4. Recommended: at least 100+ audio files for good training\")\n",
    "\n",
    "# Configure your audio data path here\n",
    "AUDIO_DATA_PATH = \"/kaggle/input/your-audio-dataset\"  # Update this path!\n",
    "\n",
    "# Check if audio data exists\n",
    "if not os.path.exists(AUDIO_DATA_PATH):\n",
    "    print(f\"Audio data path not found: {AUDIO_DATA_PATH}\")\n",
    "    print(\"Please update AUDIO_DATA_PATH to point to your uploaded audio files\")\n",
    "    print(\"Creating minimal synthetic dataset for demonstration...\")\n",
    "    \n",
    "    # Create a minimal synthetic dataset as fallback\n",
    "    class FallbackDataset(Dataset):\n",
    "        def __init__(self, num_samples=1000):\n",
    "            self.num_samples = num_samples\n",
    "            \n",
    "        def __len__(self):\n",
    "            return self.num_samples\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            # Generate simple synthetic data\n",
    "            source_a = torch.randn(SPECTROGRAM_HEIGHT, SPECTROGRAM_WIDTH) * 0.5\n",
    "            source_b = torch.randn(SPECTROGRAM_HEIGHT, SPECTROGRAM_WIDTH) * 0.5\n",
    "            noise = torch.randn(SPECTROGRAM_HEIGHT, SPECTROGRAM_WIDTH) * 0.05\n",
    "            \n",
    "            # Create crossfade target\n",
    "            target = torch.zeros_like(source_a)\n",
    "            for i in range(SPECTROGRAM_WIDTH):\n",
    "                alpha = i / SPECTROGRAM_WIDTH\n",
    "                target[:, i] = (1 - alpha) * source_a[:, i] + alpha * source_b[:, i]\n",
    "            \n",
    "            inputs = torch.stack([source_a, source_b, noise], dim=0)\n",
    "            target = target.unsqueeze(0)\n",
    "            \n",
    "            return inputs, target\n",
    "    \n",
    "    # Use fallback dataset\n",
    "    train_dataset = FallbackDataset(TRAIN_SAMPLES)\n",
    "    val_dataset = FallbackDataset(VAL_SAMPLES)\n",
    "    \n",
    "    print(f\"Using fallback synthetic dataset\")\n",
    "    print(f\"   Training samples: {len(train_dataset):,}\")\n",
    "    print(f\"   Validation samples: {len(val_dataset):,}\")\n",
    "    \n",
    "else:\n",
    "    # Use real audio dataset\n",
    "    print(f\"Found audio data at: {AUDIO_DATA_PATH}\")\n",
    "    \n",
    "    try:\n",
    "        # Create datasets\n",
    "        train_dataset = AudioTransitionDataset(\n",
    "            audio_dir=AUDIO_DATA_PATH,\n",
    "            spectrogram_size=(SPECTROGRAM_HEIGHT, SPECTROGRAM_WIDTH),\n",
    "            sample_rate=SAMPLE_RATE,\n",
    "            hop_length=HOP_LENGTH,\n",
    "            split='train',\n",
    "            val_split=0.2\n",
    "        )\n",
    "        \n",
    "        val_dataset = AudioTransitionDataset(\n",
    "            audio_dir=AUDIO_DATA_PATH,\n",
    "            spectrogram_size=(SPECTROGRAM_HEIGHT, SPECTROGRAM_WIDTH),\n",
    "            sample_rate=SAMPLE_RATE,\n",
    "            hop_length=HOP_LENGTH,\n",
    "            split='val',\n",
    "            val_split=0.2\n",
    "        )\n",
    "        \n",
    "        print(f\"Real audio dataset created successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating audio dataset: {e}\")\n",
    "        print(\"Falling back to synthetic dataset...\")\n",
    "        \n",
    "        # Fallback to synthetic\n",
    "        train_dataset = FallbackDataset(TRAIN_SAMPLES)\n",
    "        val_dataset = FallbackDataset(VAL_SAMPLES)\n",
    "\n",
    "# Test dataset\n",
    "test_inputs, test_targets = train_dataset[0]\n",
    "print(f\"Dataset ready!\")\n",
    "print(f\"   Training samples: {len(train_dataset):,}\")\n",
    "print(f\"   Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"   Input shape: {test_inputs.shape}\")\n",
    "print(f\"   Target shape: {test_targets.shape}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Data loaders created!\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "\n",
    "del test_inputs, test_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a51cae",
   "metadata": {},
   "source": [
    "### How to Upload Your Audio Data to Kaggle:\n",
    "\n",
    "1. **Prepare Your Audio Files:**\n",
    "   - Organize your audio files in folders (e.g., by genre, artist, etc.)\n",
    "   - Supported formats: WAV, MP3, FLAC, M4A\n",
    "   - Recommended: 100+ audio files for good training results\n",
    "   - Each file should be at least 15 seconds long\n",
    "\n",
    "2. **Upload to Kaggle:**\n",
    "   - Click \"Add Data\" → \"Upload\" in the right panel\n",
    "   - Upload your audio folder as a ZIP file\n",
    "   - Wait for upload to complete\n",
    "   - Note the dataset path (usually `/kaggle/input/your-dataset-name/`)\n",
    "\n",
    "3. **Update the Path:**\n",
    "   - Modify the `AUDIO_DATA_PATH` variable below\n",
    "   - Run the dataset creation cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c47ba4",
   "metadata": {},
   "source": [
    "## Initialize Training Components\n",
    "\n",
    "Set up the trainer class with model, optimizer, loss function, and data loaders for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4584acec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = ProductionUNet(\n",
    "    in_channels=IN_CHANNELS,\n",
    "    out_channels=OUT_CHANNELS,\n",
    "    model_dim=MODEL_DIM\n",
    ").to(device)\n",
    "\n",
    "print(f\" Model initialized!\")\n",
    "print(f\"   Parameters: {model.count_parameters():,}\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Initialize optimizer with learning rate scheduling\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=1e-5,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "def get_lr_schedule_with_warmup(optimizer, warmup_epochs, total_epochs):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            # Warmup phase\n",
    "            return (epoch + 1) / warmup_epochs\n",
    "        else:\n",
    "            # Cosine annealing phase\n",
    "            progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
    "            return 0.5 * (1 + np.cos(np.pi * progress))\n",
    "    \n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "scheduler = get_lr_schedule_with_warmup(optimizer, WARMUP_EPOCHS, NUM_EPOCHS)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Initialize tensorboard\n",
    "writer = SummaryWriter('logs/kaggle_training')\n",
    "\n",
    "print(\" Training components initialized!\")\n",
    "print(f\"   Optimizer: AdamW\")\n",
    "print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   Scheduler: Warmup + Cosine Annealing\")\n",
    "print(f\"   Loss Function: MSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab62d50",
   "metadata": {},
   "source": [
    "##  Training Loop Implementation\n",
    "\n",
    "Execute the main training loop with epoch-by-epoch training, validation, and progress monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00574b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training state\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "learning_rates = []\n",
    "\n",
    "print(\" Starting training...\")\n",
    "print(f\" Epochs: {NUM_EPOCHS} | Batch size: {BATCH_SIZE}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    current_epoch = epoch + 1\n",
    "    \n",
    "    print(f\"\\n📈 Epoch {current_epoch}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    num_train_batches = len(train_loader)\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "        # Progress logging\n",
    "        if batch_idx % 200 == 0:\n",
    "            progress = batch_idx / num_train_batches * 100\n",
    "            print(f\"   Batch {batch_idx}/{num_train_batches} ({progress:.1f}%) - Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    avg_train_loss = epoch_train_loss / num_train_batches\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    num_val_batches = len(val_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            epoch_val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = epoch_val_loss / num_val_batches\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    learning_rates.append(current_lr)\n",
    "    \n",
    "    # Log to tensorboard\n",
    "    writer.add_scalar('Loss/Train', avg_train_loss, current_epoch)\n",
    "    writer.add_scalar('Loss/Validation', avg_val_loss, current_epoch)\n",
    "    writer.add_scalar('Learning_Rate', current_lr, current_epoch)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\" Train Loss: {avg_train_loss:.6f}\")\n",
    "    print(f\" Val Loss: {avg_val_loss:.6f}\")\n",
    "    print(f\" Learning Rate: {current_lr:.2e}\")\n",
    "    \n",
    "    # Save best model\n",
    "    is_best = avg_val_loss < best_val_loss\n",
    "    if is_best:\n",
    "        best_val_loss = avg_val_loss\n",
    "        # Save best model\n",
    "        best_checkpoint = {\n",
    "            'epoch': current_epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'model_config': {\n",
    "            'in_channels': IN_CHANNELS,\n",
    "            'out_channels': OUT_CHANNELS,\n",
    "            'model_dim': MODEL_DIM\n",
    "            }\n",
    "        }\n",
    "        torch.save(best_checkpoint, 'checkpoints/best_model_kaggle.pt')\n",
    "        print(f\" New best model saved! (Val Loss: {best_val_loss:.6f})\")\n",
    "        \n",
    "        # Save regular checkpoint every 5 epochs\n",
    "        if current_epoch % 5 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': current_epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'model_config': {\n",
    "            'in_channels': IN_CHANNELS,\n",
    "            'out_channels': OUT_CHANNELS,\n",
    "            'model_dim': MODEL_DIM\n",
    "            }\n",
    "        }\n",
    "        torch.save(checkpoint, f'checkpoints/model_epoch_{current_epoch}.pt')\n",
    "        print(f\"💾 Checkpoint saved: model_epoch_{current_epoch}.pt\")\n",
    "        \n",
    "        # Memory cleanup\n",
    "        if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Training completed\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n Training completed!\")\n",
    "    print(f\" Total time: {total_time:.1f}s ({total_time/60:.1f}m)\")\n",
    "    print(f\" Best validation loss: {best_val_loss:.6f}\")\n",
    "\n",
    "    # Save final checkpoint\n",
    "    final_checkpoint = {\n",
    "        'epoch': NUM_EPOCHS,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'learning_rates': learning_rates,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'total_training_time': total_time,\n",
    "        'model_config': {\n",
    "        'in_channels': IN_CHANNELS,\n",
    "        'out_channels': OUT_CHANNELS,\n",
    "        'model_dim': MODEL_DIM\n",
    "        }\n",
    "    }\n",
    "    torch.save(final_checkpoint, 'checkpoints/final_model_kaggle.pt')\n",
    "    print(\" Final checkpoint saved: final_model_kaggle.pt\")\n",
    "\n",
    "    # Close tensorboard writer\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a35f51d",
   "metadata": {},
   "source": [
    "##  Model Evaluation and Validation\n",
    "\n",
    "Evaluate model performance on validation data and visualize training metrics and loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65a8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Loss curves\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "axes[0,0].plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "axes[0,0].plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "axes[0,0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Loss')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate schedule\n",
    "axes[0,1].plot(epochs, learning_rates, 'g-', linewidth=2)\n",
    "axes[0,1].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_xlabel('Epoch')\n",
    "axes[0,1].set_ylabel('Learning Rate')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Generate sample predictions\n",
    "print(\" Generating sample predictions...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a batch from validation set\n",
    "    sample_inputs, sample_targets = next(iter(val_loader))\n",
    "    sample_inputs = sample_inputs.to(device)\n",
    "    sample_targets = sample_targets.to(device)\n",
    "    \n",
    "    # Generate predictions\n",
    "    sample_predictions = model(sample_inputs)\n",
    "    \n",
    "    # Move to CPU for visualization\n",
    "    sample_inputs = sample_inputs.cpu()\n",
    "    sample_targets = sample_targets.cpu()\n",
    "    sample_predictions = sample_predictions.cpu()\n",
    "\n",
    "# Visualize sample predictions\n",
    "sample_idx = 0\n",
    "source_a = sample_inputs[sample_idx, 0].numpy()\n",
    "source_b = sample_inputs[sample_idx, 1].numpy()\n",
    "target = sample_targets[sample_idx, 0].numpy()\n",
    "prediction = sample_predictions[sample_idx, 0].numpy()\n",
    "\n",
    "# Plot source A\n",
    "im1 = axes[1,0].imshow(source_a, aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[1,0].set_title('Source A (House)', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Time Frames')\n",
    "axes[1,0].set_ylabel('Frequency Bins')\n",
    "plt.colorbar(im1, ax=axes[1,0], fraction=0.046, pad=0.04)\n",
    "\n",
    "# Plot generated transition vs target\n",
    "im2 = axes[1,1].imshow(prediction, aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[1,1].set_title('Generated Transition', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Time Frames')\n",
    "axes[1,1].set_ylabel('Frequency Bins')\n",
    "plt.colorbar(im2, ax=axes[1,1], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/training_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate final metrics\n",
    "final_train_loss = train_losses[-1]\n",
    "final_val_loss = val_losses[-1]\n",
    "improvement = (train_losses[0] - final_train_loss) / train_losses[0] * 100\n",
    "\n",
    "print(f\"\\n Training Summary:\")\n",
    "print(f\"   Initial Training Loss: {train_losses[0]:.6f}\")\n",
    "print(f\"   Final Training Loss: {final_train_loss:.6f}\")\n",
    "print(f\"   Final Validation Loss: {final_val_loss:.6f}\")\n",
    "print(f\"   Best Validation Loss: {best_val_loss:.6f}\")\n",
    "print(f\"   Training Improvement: {improvement:.1f}%\")\n",
    "print(f\"   Total Parameters: {model.count_parameters():,}\")\n",
    "print(f\"   Training Time: {total_time/60:.1f} minutes\")\n",
    "\n",
    "# Model quality assessment\n",
    "if best_val_loss < 0.1:\n",
    "    quality = \"EXCELLENT\"\n",
    "elif best_val_loss < 0.2:\n",
    "    quality = \"GOOD\"\n",
    "elif best_val_loss < 0.3:\n",
    "    quality = \"FAIR\"\n",
    "else:\n",
    "    quality = \"NEEDS IMPROVEMENT\"\n",
    "\n",
    "print(f\"\\nModel Quality: {quality}\")\n",
    "print(f\"Best Validation Loss: {best_val_loss:.6f}\")\n",
    "\n",
    "# Save evaluation results\n",
    "evaluation_results = {\n",
    "    'final_train_loss': final_train_loss,\n",
    "    'final_val_loss': final_val_loss,\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'training_improvement_percent': improvement,\n",
    "    'total_parameters': model.count_parameters(),\n",
    "    'training_time_minutes': total_time/60,\n",
    "    'quality_assessment': quality,\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'learning_rates': learning_rates\n",
    "}\n",
    "\n",
    "torch.save(evaluation_results, 'outputs/evaluation_results.pt')\n",
    "print(f\"\\n Evaluation results saved to outputs/evaluation_results.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af697f4b",
   "metadata": {},
   "source": [
    "## Generate Sample Audio Transitions\n",
    "\n",
    "Let's generate some actual audio transitions to test our trained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3932c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio generation class for creating actual audio files\n",
    "class AudioGenerator:\n",
    "    def __init__(self, model, device, sample_rate=22050):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "    def spectrogram_to_audio(self, spectrogram):\n",
    "        \"\"\"Convert spectrogram back to audio using Griffin-Lim algorithm\"\"\"\n",
    "        # Ensure spectrogram is 2D\n",
    "        if len(spectrogram.shape) == 3:\n",
    "            spectrogram = spectrogram.squeeze(0)\n",
    "        \n",
    "        # Convert to magnitude spectrogram\n",
    "        magnitude = np.exp(spectrogram) - 1e-6\n",
    "        \n",
    "        # Use Griffin-Lim to reconstruct audio\n",
    "        audio = librosa.griffinlim(magnitude, \n",
    "                                 hop_length=512, \n",
    "                                 win_length=1024,\n",
    "                                 n_iter=32)\n",
    "        \n",
    "        return audio\n",
    "    \n",
    "    def create_synthetic_track(self, style='house', duration=4.0):\n",
    "        \"\"\"Create a synthetic track segment\"\"\"\n",
    "        n_frames = int(duration * self.sample_rate / 512)  # 512 is hop_length\n",
    "        \n",
    "        if style == 'house':\n",
    "            # House: 120-128 BPM, 4/4 kick pattern\n",
    "            fundamental = np.random.uniform(80, 120)  # Hz\n",
    "            harmonics = [1, 2, 3, 4, 5]\n",
    "            kick_pattern = np.array([1, 0, 0, 0] * (n_frames // 4))[:n_frames]\n",
    "        else:\n",
    "            # Techno: 128-140 BPM, more aggressive\n",
    "            fundamental = np.random.uniform(100, 150)  # Hz\n",
    "            harmonics = [1, 2, 3, 5, 7]\n",
    "            kick_pattern = np.array([1, 0, 1, 0] * (n_frames // 4))[:n_frames]\n",
    "        \n",
    "        # Create frequency content\n",
    "        n_mels = 128\n",
    "        spectrogram = np.random.normal(0, 0.1, (n_mels, n_frames))\n",
    "        \n",
    "        # Add rhythmic elements\n",
    "        for i, intensity in enumerate(kick_pattern):\n",
    "            if intensity > 0:\n",
    "                spectrogram[20:40, i] += intensity * np.random.uniform(0.5, 1.0)\n",
    "        \n",
    "        # Add harmonic content\n",
    "        for harmonic in harmonics:\n",
    "            freq_bin = min(int(harmonic * fundamental / 22050 * 128), 127)\n",
    "            spectrogram[freq_bin-2:freq_bin+2, :] += np.random.uniform(0.2, 0.4)\n",
    "        \n",
    "        # Smooth and normalize\n",
    "        from scipy.ndimage import gaussian_filter\n",
    "        spectrogram = gaussian_filter(spectrogram, sigma=0.5)\n",
    "        spectrogram = np.clip(spectrogram, 0, None)\n",
    "        \n",
    "        return np.log(spectrogram + 1e-6)\n",
    "    \n",
    "    def generate_transition(self, style_a='house', style_b='techno', duration=4.0):\n",
    "        \"\"\"Generate a transition between two styles\"\"\"\n",
    "        # Create source tracks\n",
    "        source_a = self.create_synthetic_track(style_a, duration)\n",
    "        source_b = self.create_synthetic_track(style_b, duration)\n",
    "        \n",
    "        # Prepare input tensor\n",
    "        input_tensor = np.stack([source_a, source_b])\n",
    "        input_tensor = torch.FloatTensor(input_tensor).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Generate transition\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            transition = self.model(input_tensor)\n",
    "            transition = transition.cpu().numpy().squeeze()\n",
    "        \n",
    "        return source_a, source_b, transition\n",
    "\n",
    "# Create audio generator\n",
    "audio_gen = AudioGenerator(model, device)\n",
    "\n",
    "print(\"🎵 Generating sample transitions...\")\n",
    "\n",
    "# Generate different style combinations\n",
    "transitions = []\n",
    "styles = [('house', 'techno'), ('techno', 'house'), ('house', 'house'), ('techno', 'techno')]\n",
    "\n",
    "for i, (style_a, style_b) in enumerate(styles):\n",
    "    print(f\"   Generating {style_a} → {style_b} transition...\")\n",
    "    source_a, source_b, transition = audio_gen.generate_transition(style_a, style_b)\n",
    "    \n",
    "    # Convert spectrograms to audio\n",
    "    audio_a = audio_gen.spectrogram_to_audio(source_a)\n",
    "    audio_b = audio_gen.spectrogram_to_audio(source_b)\n",
    "    audio_transition = audio_gen.spectrogram_to_audio(transition)\n",
    "    \n",
    "    # Save audio files\n",
    "    sf.write(f'outputs/source_a_{style_a}_{i}.wav', audio_a, 22050)\n",
    "    sf.write(f'outputs/source_b_{style_b}_{i}.wav', audio_b, 22050)\n",
    "    sf.write(f'outputs/transition_{style_a}_to_{style_b}_{i}.wav', audio_transition, 22050)\n",
    "    \n",
    "    transitions.append({\n",
    "        'styles': (style_a, style_b),\n",
    "        'spectrograms': (source_a, source_b, transition),\n",
    "        'audio': (audio_a, audio_b, audio_transition)\n",
    "    })\n",
    "\n",
    "print(f\"Generated {len(transitions)} sample transitions!\")\n",
    "print(\"Audio files saved in outputs/ directory\")\n",
    "\n",
    "# Visualize the generated transitions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "for i, transition_data in enumerate(transitions[:2]):  # Show first 2 transitions\n",
    "    source_a, source_b, transition = transition_data['spectrograms']\n",
    "    style_a, style_b = transition_data['styles']\n",
    "    \n",
    "    # Plot spectrograms\n",
    "    im1 = axes[i,0].imshow(source_a, aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[i,0].set_title(f'Source A ({style_a.title()})', fontweight='bold')\n",
    "    axes[i,0].set_ylabel('Frequency Bins')\n",
    "    \n",
    "    im2 = axes[i,1].imshow(transition, aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[i,1].set_title(f'Generated Transition', fontweight='bold')\n",
    "    \n",
    "    im3 = axes[i,2].imshow(source_b, aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[i,2].set_title(f'Source B ({style_b.title()})', fontweight='bold')\n",
    "    \n",
    "    if i == 1:  # Bottom row\n",
    "        for ax in axes[i,:]:\n",
    "            ax.set_xlabel('Time Frames')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/generated_transitions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSample transitions generated! You can download the audio files to listen to them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d3ddb8",
   "metadata": {},
   "source": [
    "## Save and Export Model\n",
    "\n",
    "Finally, let's save our trained model and create a deployment package!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937d6d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive model export\n",
    "print(\"Preparing model for export...\")\n",
    "\n",
    "# 1. Save the complete model\n",
    "model_export = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'in_channels': 2,\n",
    "        'out_channels': 1,\n",
    "        'features': [64, 128, 256, 512],\n",
    "        'model_type': 'ProductionUNet',\n",
    "        'total_parameters': model.count_parameters(),\n",
    "        'input_shape': [2, 128, 512],\n",
    "        'output_shape': [1, 128, 512]\n",
    "    },\n",
    "    'training_info': {\n",
    "        'epochs_trained': config['num_epochs'],\n",
    "        'final_train_loss': final_train_loss,\n",
    "        'final_val_loss': final_val_loss,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'training_time_minutes': total_time/60,\n",
    "        'optimizer': 'AdamW',\n",
    "        'learning_rate': config['learning_rate'],\n",
    "        'batch_size': config['batch_size']\n",
    "    },\n",
    "    'training_history': {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'learning_rates': learning_rates\n",
    "    },\n",
    "    'metadata': {\n",
    "        'created_date': str(datetime.now()),\n",
    "        'framework': 'PyTorch',\n",
    "        'device_trained': str(device),\n",
    "        'kaggle_notebook': True,\n",
    "        'model_version': '1.0'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save complete model\n",
    "torch.save(model_export, 'outputs/dj_transition_model_complete.pt')\n",
    "print(\"Complete model saved to outputs/dj_transition_model_complete.pt\")\n",
    "\n",
    "# 2. Save just the model weights (smaller file)\n",
    "torch.save(model.state_dict(), 'outputs/dj_transition_model_weights.pt')\n",
    "print(\"Model weights saved to outputs/dj_transition_model_weights.pt\")\n",
    "\n",
    "# 3. Export model in ONNX format for deployment\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, 2, 128, 512).to(device)\n",
    "\n",
    "try:\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        'outputs/dj_transition_model.onnx',\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['source_tracks'],\n",
    "        output_names=['transition'],\n",
    "        dynamic_axes={\n",
    "            'source_tracks': {0: 'batch_size'},\n",
    "            'transition': {0: 'batch_size'}\n",
    "        }\n",
    "    )\n",
    "    print(\"ONNX model saved to outputs/dj_transition_model.onnx\")\n",
    "except Exception as e:\n",
    "    print(f\"ONNX export failed: {e}\")\n",
    "\n",
    "# 4. Create deployment configuration\n",
    "deployment_config = {\n",
    "    'model_info': {\n",
    "        'name': 'DJNet Transition Generator',\n",
    "        'version': '1.0',\n",
    "        'description': 'Deep learning model for generating smooth transitions between DJ tracks',\n",
    "        'input_format': 'Mel-spectrogram pairs (2, 128, 512)',\n",
    "        'output_format': 'Transition mel-spectrogram (1, 128, 512)',\n",
    "        'sampling_rate': 22050,\n",
    "        'hop_length': 512,\n",
    "        'n_mels': 128\n",
    "    },\n",
    "    'inference': {\n",
    "        'preprocessing': {\n",
    "            'normalize': True,\n",
    "            'log_scale': True,\n",
    "            'clamp_min': 1e-6\n",
    "        },\n",
    "        'postprocessing': {\n",
    "            'griffin_lim_iterations': 32,\n",
    "            'hop_length': 512,\n",
    "            'win_length': 1024\n",
    "        }\n",
    "    },\n",
    "    'performance': {\n",
    "        'parameters': model.count_parameters(),\n",
    "        'inference_time_gpu_ms': '~50',  # Estimated\n",
    "        'memory_requirement_mb': '~500',  # Estimated\n",
    "        'recommended_device': 'GPU (CUDA)',\n",
    "        'minimum_device': 'CPU'\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('outputs/deployment_config.json', 'w') as f:\n",
    "    json.dump(deployment_config, f, indent=2)\n",
    "\n",
    "print(\"Deployment config saved to outputs/deployment_config.json\")\n",
    "\n",
    "# 5. Create a simple inference script\n",
    "inference_script = '''\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load the trained model\"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    \n",
    "    # Reconstruct model architecture\n",
    "    from your_model_file import ProductionUNet  # Replace with actual import\n",
    "    model = ProductionUNet(in_channels=2, out_channels=1)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def audio_to_spectrogram(audio_path, sr=22050, n_mels=128, hop_length=512):\n",
    "    \"\"\"Convert audio file to mel-spectrogram\"\"\"\n",
    "    audio, _ = librosa.load(audio_path, sr=sr)\n",
    "    \n",
    "    # Create mel-spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio, sr=sr, n_mels=n_mels, hop_length=hop_length\n",
    "    )\n",
    "    \n",
    "    # Convert to log scale\n",
    "    log_mel = np.log(mel_spec + 1e-6)\n",
    "    \n",
    "    return log_mel\n",
    "\n",
    "def spectrogram_to_audio(spectrogram, sr=22050, hop_length=512):\n",
    "    \"\"\"Convert mel-spectrogram back to audio\"\"\"\n",
    "    # Convert from log scale\n",
    "    mel_spec = np.exp(spectrogram) - 1e-6\n",
    "    \n",
    "    # Reconstruct audio using Griffin-Lim\n",
    "    audio = librosa.griffinlim(mel_spec, hop_length=hop_length, n_iter=32)\n",
    "    \n",
    "    return audio\n",
    "\n",
    "def generate_transition(model, audio_path_a, audio_path_b, output_path):\n",
    "    \"\"\"Generate transition between two audio files\"\"\"\n",
    "    # Load and convert audio to spectrograms\n",
    "    spec_a = audio_to_spectrogram(audio_path_a)\n",
    "    spec_b = audio_to_spectrogram(audio_path_b)\n",
    "    \n",
    "    # Prepare input tensor\n",
    "    input_tensor = torch.FloatTensor(np.stack([spec_a, spec_b])).unsqueeze(0)\n",
    "    \n",
    "    # Generate transition\n",
    "    with torch.no_grad():\n",
    "        transition = model(input_tensor)\n",
    "        transition = transition.cpu().numpy().squeeze()\n",
    "    \n",
    "    # Convert back to audio\n",
    "    audio_transition = spectrogram_to_audio(transition)\n",
    "    \n",
    "    # Save audio\n",
    "    sf.write(output_path, audio_transition, 22050)\n",
    "    \n",
    "    return audio_transition\n",
    "\n",
    "# Example usage:\n",
    "# model = load_model('dj_transition_model_complete.pt')\n",
    "# generate_transition(model, 'track_a.wav', 'track_b.wav', 'transition.wav')\n",
    "'''\n",
    "\n",
    "with open('outputs/inference_example.py', 'w') as f:\n",
    "    f.write(inference_script)\n",
    "\n",
    "print(\"Inference script saved to outputs/inference_example.py\")\n",
    "\n",
    "# 6. Create README\n",
    "readme_content = f'''# DJNet Transition Generator\n",
    "\n",
    "## Model Overview\n",
    "This is a trained U-Net model for generating smooth transitions between DJ tracks.\n",
    "\n",
    "### Model Details\n",
    "- **Architecture**: U-Net with skip connections\n",
    "- **Parameters**: {model.count_parameters():,}\n",
    "- **Input**: Pair of mel-spectrograms (2, 128, 512)\n",
    "- **Output**: Transition mel-spectrogram (1, 128, 512)\n",
    "- **Training**: {config['num_epochs']} epochs on synthetic data\n",
    "\n",
    "### Performance\n",
    "- **Final Training Loss**: {final_train_loss:.6f}\n",
    "- **Final Validation Loss**: {final_val_loss:.6f}\n",
    "- **Best Validation Loss**: {best_val_loss:.6f}\n",
    "- **Training Time**: {total_time/60:.1f} minutes\n",
    "\n",
    "### Files Included\n",
    "- `dj_transition_model_complete.pt` - Full model with training info\n",
    "- `dj_transition_model_weights.pt` - Model weights only\n",
    "- `dj_transition_model.onnx` - ONNX format for deployment\n",
    "- `deployment_config.json` - Configuration for deployment\n",
    "- `inference_example.py` - Example inference script\n",
    "- `evaluation_results.pt` - Training metrics and results\n",
    "\n",
    "### Quick Start\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "checkpoint = torch.load('dj_transition_model_complete.pt')\n",
    "model = ProductionUNet(in_channels=2, out_channels=1)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Generate transition (see inference_example.py for full code)\n",
    "```\n",
    "\n",
    "### Requirements\n",
    "- PyTorch >= 1.9.0\n",
    "- librosa >= 0.8.0\n",
    "- numpy >= 1.19.0\n",
    "- soundfile >= 0.10.0\n",
    "\n",
    "Generated on: {str(datetime.now())}\n",
    "'''\n",
    "\n",
    "with open('outputs/README.md', 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"✅ README saved to outputs/README.md\")\n",
    "\n",
    "# 7. Display file summary\n",
    "print(f\"\\n📦 Export Summary:\")\n",
    "print(f\"   📁 outputs/dj_transition_model_complete.pt ({os.path.getsize('outputs/dj_transition_model_complete.pt')/1024/1024:.1f} MB)\")\n",
    "print(f\"   📁 outputs/dj_transition_model_weights.pt ({os.path.getsize('outputs/dj_transition_model_weights.pt')/1024/1024:.1f} MB)\")\n",
    "if os.path.exists('outputs/dj_transition_model.onnx'):\n",
    "    print(f\"   📁 outputs/dj_transition_model.onnx ({os.path.getsize('outputs/dj_transition_model.onnx')/1024/1024:.1f} MB)\")\n",
    "print(f\"   📁 outputs/deployment_config.json\")\n",
    "print(f\"   📁 outputs/inference_example.py\")\n",
    "print(f\"   📁 outputs/README.md\")\n",
    "print(f\"   📁 outputs/evaluation_results.pt\")\n",
    "\n",
    "print(f\"\\n🎉 Model training and export complete!\")\n",
    "print(f\"💡 Download the outputs/ folder to get all model files\")\n",
    "print(f\"🚀 Ready for deployment and production use!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
