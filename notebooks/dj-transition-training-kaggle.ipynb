{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12691404,"sourceType":"datasetVersion","datasetId":8020464},{"sourceId":12694536,"sourceType":"datasetVersion","datasetId":8022657},{"sourceId":512348,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":405561,"modelId":423471}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"effbc1c1","cell_type":"markdown","source":"# DJ Transition Generation Training - Kaggle Edition\n\nThis notebook trains a deep learning model to generate smooth transitions between electronic music tracks using a U-Net architecture. \n\n## Instructions\n1. Enable GPU accelerator in Kaggle (Settings ‚Üí Accelerator ‚Üí GPU)\n2. Run all cells sequentially\n3. Training will take approximately 2-3 hours on Kaggle GPU\n4. Download the trained model from the output section","metadata":{}},{"id":"934b162a","cell_type":"markdown","source":"## Install Dependencies\n\nInstall required packages that may not be available in Kaggle's default environment.","metadata":{}},{"id":"39ec057d","cell_type":"code","source":"# Install required packages\n!pip install tensorboard -q\n!pip install soundfile -q\n!pip install librosa -q\n!pip install scikit-image -q\n\nprint(\"All dependencies installed successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6ed40a17","cell_type":"markdown","source":"## üîß Setup Environment and Import Libraries\n\nImport all necessary libraries and configure GPU settings for optimal performance.","metadata":{}},{"id":"668a2ea8","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.tensorboard import SummaryWriter\nimport os\nimport sys\nimport time\nimport gc\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport soundfile as sf\nfrom IPython.display import display, Audio, HTML\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Optimize PyTorch performance\ntorch.backends.cudnn.benchmark = True if torch.cuda.is_available() else False\ntorch.backends.cudnn.deterministic = False\n\n# Check GPU availability\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\nelse:\n    print(\"No GPU available, using CPU (training will be much slower)\")\n\n# Create necessary directories\nos.makedirs('checkpoints', exist_ok=True)\nos.makedirs('logs', exist_ok=True)\nos.makedirs('outputs', exist_ok=True)\n\nprint(\"Environment setup complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e08dcab3","cell_type":"markdown","source":"## ‚öôÔ∏è Load Configuration\n\nDefine model hyperparameters and training configuration optimized for Kaggle environment.","metadata":{}},{"id":"e2b6bc2c","cell_type":"code","source":"# Configuration Parameters - Optimized for Kaggle\nSAMPLE_RATE = 22050\nN_FFT = 2048\nHOP_LENGTH = 512\nN_MELS = 128\nSPECTROGRAM_HEIGHT = 128\nSPECTROGRAM_WIDTH = 512\nSEGMENT_DURATION = 12.0  # Use 12s effective duration (cut last 3s from 15s)\n\n# Model Configuration\nIN_CHANNELS = 3  # [source_a, source_b, noise]\nOUT_CHANNELS = 1  # transition\nMODEL_DIM = 512\n\n# Training Configuration - Optimized for Kaggle GPU\nBATCH_SIZE = 28  # Increased batch size for better GPU utilization\nGRADIENT_ACCUMULATION_STEPS = 2\nLEARNING_RATE = 1e-4\nNUM_EPOCHS = 30  # Reduced epochs for Kaggle time limits\nWARMUP_EPOCHS = 3\n\n# Dataset Configuration\nTRAIN_SAMPLES = 15000  # Will be determined by your uploaded files\nVAL_SAMPLES = 2000     # Will be determined by your uploaded files\n\nprint(\" Configuration loaded:\")\nprint(f\"   Batch Size: {BATCH_SIZE}\")\nprint(f\"   Epochs: {NUM_EPOCHS}\")\nprint(f\"   Learning Rate: {LEARNING_RATE}\")\nprint(f\"   Model Dimension: {MODEL_DIM}\")\nprint(f\"   Training Samples: {TRAIN_SAMPLES}\")\nprint(f\"   Validation Samples: {VAL_SAMPLES}\")\nprint(f\"   Spectrogram Size: {SPECTROGRAM_HEIGHT}x{SPECTROGRAM_WIDTH}\")\nprint(f\"   Segment Duration (effective): {SEGMENT_DURATION}s (cut last 3s from 15s)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8c287012","cell_type":"markdown","source":"## Define Model Architecture\n\nImplement the ProductionUNet model with encoder-decoder architecture for DJ transition generation.","metadata":{}},{"id":"49e394dc","cell_type":"code","source":"class ProductionUNet(nn.Module):\n    \"\"\"\n    Production U-Net model for generating DJ transitions\n    \"\"\"\n    \n    def __init__(self, in_channels=3, out_channels=1, model_dim=512):\n        super().__init__()\n        \n        # Encoder path\n        self.enc1 = self._make_encoder_block(in_channels, 64)\n        self.enc2 = self._make_encoder_block(64, 128) \n        self.enc3 = self._make_encoder_block(128, 256)\n        self.enc4 = self._make_encoder_block(256, 512)\n        \n        # Bottleneck\n        self.bottleneck = nn.Sequential(\n            nn.Conv2d(512, model_dim, 3, padding=1),\n            nn.BatchNorm2d(model_dim),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(model_dim, model_dim, 3, padding=1),\n            nn.BatchNorm2d(model_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.2)\n        )\n        \n        # Decoder path\n        self.upconv4 = nn.ConvTranspose2d(model_dim, 512, 2, stride=2)\n        self.dec4 = self._make_decoder_block(512 + 256, 512)\n        \n        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n        self.dec3 = self._make_decoder_block(256 + 128, 256)\n        \n        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        self.dec2 = self._make_decoder_block(128 + 64, 128)\n        \n        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n        self.dec1 = self._make_decoder_block(64, 64)\n        \n        # Final output layer\n        self.final = nn.Sequential(\n            nn.Conv2d(64, 32, 3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, out_channels, 1),\n            nn.Tanh()\n        )\n        \n    def _make_encoder_block(self, in_channels, out_channels):\n        \"\"\"Create encoder block with convolution, normalization, and pooling\"\"\"\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2)\n        )\n    \n    def _make_decoder_block(self, in_channels, out_channels):\n        \"\"\"Create decoder block with convolution, normalization, and dropout\"\"\"\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.1)\n        )\n    \n    def forward(self, x):\n        \"\"\"Forward pass through the U-Net\"\"\"\n        # Encoder path with skip connections\n        e1 = self.enc1(x)      # [B, 64, H/2, W/2]\n        e2 = self.enc2(e1)     # [B, 128, H/4, W/4]\n        e3 = self.enc3(e2)     # [B, 256, H/8, W/8]\n        e4 = self.enc4(e3)     # [B, 512, H/16, W/16]\n        \n        # Bottleneck\n        bottleneck = self.bottleneck(e4)  # [B, model_dim, H/16, W/16]\n        \n        # Decoder path with skip connections\n        d4 = self.upconv4(bottleneck)     # [B, 512, H/8, W/8]\n        d4 = torch.cat([d4, e3], dim=1)   # [B, 512+256, H/8, W/8]\n        d4 = self.dec4(d4)                # [B, 512, H/8, W/8]\n        \n        d3 = self.upconv3(d4)             # [B, 256, H/4, W/4]\n        d3 = torch.cat([d3, e2], dim=1)   # [B, 256+128, H/4, W/4]\n        d3 = self.dec3(d3)                # [B, 256, H/4, W/4]\n        \n        d2 = self.upconv2(d3)             # [B, 128, H/2, W/2]\n        d2 = torch.cat([d2, e1], dim=1)   # [B, 128+64, H/2, W/2]\n        d2 = self.dec2(d2)                # [B, 128, H/2, W/2]\n        \n        d1 = self.upconv1(d2)             # [B, 64, H, W]\n        d1 = self.dec1(d1)                # [B, 64, H, W]\n        \n        # Final output\n        output = self.final(d1)           # [B, out_channels, H, W]\n        \n        return output\n    \n    def count_parameters(self):\n        \"\"\"Count the number of trainable parameters\"\"\"\n        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n\n# Test model creation\ntest_model = ProductionUNet(\n    in_channels=IN_CHANNELS,\n    out_channels=OUT_CHANNELS,\n    model_dim=MODEL_DIM\n)\n\nprint(f\"Model created successfully!\")\nprint(f\"Total parameters: {test_model.count_parameters():,}\")\nprint(f\"Model size: ~{test_model.count_parameters() * 4 / 1024**2:.1f} MB\")\n\n# Test forward pass\ntest_input = torch.randn(1, IN_CHANNELS, SPECTROGRAM_HEIGHT, SPECTROGRAM_WIDTH)\nwith torch.no_grad():\n    test_output = test_model(test_input)\nprint(f\"Model forward pass test successful!\")\nprint(f\"   Input shape: {test_input.shape}\")\nprint(f\"   Output shape: {test_output.shape}\")\n\ndel test_model, test_input, test_output\ntorch.cuda.empty_cache() if torch.cuda.is_available() else None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"fdf313f3","cell_type":"markdown","source":"## üìä Load Your Own Dataset\n\nUpload and process your own audio files for training the DJ transition model.","metadata":{}},{"id":"cc44f027","cell_type":"code","source":"import librosa\nimport glob\nfrom pathlib import Path\nimport random\n\nclass AudioTransitionDataset(Dataset):\n    \"\"\"\n    Dataset for training DJ transition models using real audio files\n    \"\"\"\n    \n    def __init__(self, audio_dir, spectrogram_size=(128, 512), \n                 sample_rate=22050, hop_length=512, split='train', val_split=0.2):\n        self.audio_dir = Path(audio_dir)\n        self.height, self.width = spectrogram_size\n        self.sample_rate = sample_rate\n        self.hop_length = hop_length\n        self.split = split\n\n        # Segment policy: dataset built with 15s segments; use only first 12s\n        self.raw_segment_duration = 15.0      # seconds (as in dataset)\n        self.effective_duration = 12.0        # seconds (cut last 3s)\n        \n        # Find all audio files\n        audio_extensions = ['*.wav', '*.mp3', '*.flac', '*.m4a']\n        audio_files = []\n        for ext in audio_extensions:\n            audio_files.extend(glob.glob(str(self.audio_dir / f\"**/{ext}\"), recursive=True))\n        \n        if not audio_files:\n            raise ValueError(f\"No audio files found in {audio_dir}\")\n        \n        # Split into train/validation\n        random.seed(42)\n        random.shuffle(audio_files)\n        split_idx = int(len(audio_files) * (1 - val_split))\n        \n        if split == 'train':\n            self.audio_files = audio_files[:split_idx]\n        else:\n            self.audio_files = audio_files[split_idx:]\n        \n        print(f\"Found {len(audio_files)} total audio files\")\n        print(f\"{split} split: {len(self.audio_files)} files\")\n        print(f\"Segment policy: take 15s from file, keep first 12s (drop last 3s)\")\n        \n    def __len__(self):\n        # Generate multiple combinations from available files\n        return len(self.audio_files) * 3  # 3x augmentation\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        Load and process audio files to create training examples\n        \n        Returns:\n            inputs: [3, H, W] tensor containing [source_a, source_b, noise]\n            target: [1, H, W] tensor containing target transition\n        \"\"\"\n        # Select two different audio files\n        file_idx = idx % len(self.audio_files)\n        source_a_path = self.audio_files[file_idx]\n        \n        # Select different file for source B\n        source_b_idx = (file_idx + 1 + (idx // len(self.audio_files))) % len(self.audio_files)\n        source_b_path = self.audio_files[source_b_idx]\n        \n        # Load and process audio files\n        try:\n            source_a_spec = self._load_audio_segment(source_a_path)\n            source_b_spec = self._load_audio_segment(source_b_path)\n            \n            # Generate noise for the third channel\n            noise = torch.randn(self.height, self.width) * 0.05\n            \n            # Create target transition\n            target = self._create_transition_target(source_a_spec, source_b_spec)\n            \n            # Stack inputs\n            inputs = torch.stack([source_a_spec, source_b_spec, noise], dim=0)\n            target = target.unsqueeze(0)  # Add channel dimension\n            \n            return inputs, target\n            \n        except Exception as e:\n            print(f\"Error loading {source_a_path} or {source_b_path}: {e}\")\n            # Fallback to synthetic data if file loading fails\n            return self._generate_fallback_data()\n    \n    def _load_audio_segment(self, audio_path):\n        \"\"\"Load and convert audio file to mel-spectrogram\"\"\"\n        # Load audio\n        audio, sr = librosa.load(audio_path, sr=self.sample_rate)\n        \n        # Extract random 15s segment (raw), then keep first 12s (drop last 3s)\n        raw_segment_samples = int(self.raw_segment_duration * self.sample_rate)   # 15s\n        effective_segment_samples = int(self.effective_duration * self.sample_rate)  # 12s\n        \n        if len(audio) > raw_segment_samples:\n            start_idx = random.randint(0, len(audio) - raw_segment_samples)\n            audio = audio[start_idx:start_idx + raw_segment_samples]\n        elif len(audio) < raw_segment_samples:\n            # Pad if too short\n            audio = np.pad(audio, (0, raw_segment_samples - len(audio)), mode='constant')\n        \n        # Keep the first 12 seconds (cut the last 3 seconds)\n        audio = audio[:effective_segment_samples]\n        \n        # Convert to mel-spectrogram\n        mel_spec = librosa.feature.melspectrogram(\n            y=audio,\n            sr=self.sample_rate,\n            n_mels=self.height,\n            hop_length=self.hop_length,\n            n_fft=2048,\n            fmin=20,\n            fmax=8000\n        )\n        \n        # Convert to log scale and normalize\n        log_mel = np.log(mel_spec + 1e-6)\n        \n        # Ensure we only keep frames corresponding to 12s before resizing\n        frames_12 = int(self.effective_duration * self.sample_rate / self.hop_length)\n        if log_mel.shape[1] > frames_12:\n            log_mel = log_mel[:, :frames_12]\n        elif log_mel.shape[1] < frames_12:\n            pad_w = frames_12 - log_mel.shape[1]\n            log_mel = np.pad(log_mel, ((0, 0), (0, pad_w)), mode='constant')\n        \n        # Resize to exact target size if needed (map 12s ‚Üí self.width=512)\n        if log_mel.shape[1] != self.width:\n            from scipy.ndimage import zoom\n            zoom_factor = self.width / log_mel.shape[1]\n            log_mel = zoom(log_mel, (1, zoom_factor), order=1)\n        \n        # Normalize to [-1, 1]\n        log_mel = (log_mel - log_mel.mean()) / (log_mel.std() + 1e-6)\n        log_mel = np.clip(log_mel, -3, 3) / 3  # Soft clipping\n        \n        return torch.FloatTensor(log_mel)\n    \n    def _create_transition_target(self, source_a, source_b):\n        \"\"\"Create target transition between two audio spectrograms\"\"\"\n        transition = torch.zeros_like(source_a)\n        \n        # Create smooth crossfade with creative elements\n        for i in range(self.width):\n            # Main crossfade curve\n            alpha = i / self.width\n            \n            # Add beat-sync variations\n            beat_sync = 0.1 * np.sin(2 * np.pi * alpha * 8)  # 8 beats across transition\n            alpha = np.clip(alpha + beat_sync, 0, 1)\n            \n            # Crossfade with frequency-dependent mixing\n            transition[:, i] = (1 - alpha) * source_a[:, i] + alpha * source_b[:, i]\n        \n        # Add transition-specific effects\n        # Enhance bass frequencies during transition\n        bass_region = slice(0, self.height // 6)\n        transition[bass_region, self.width//3:2*self.width//3] *= 1.2\n        \n        # Add high-frequency sweep\n        for i in range(self.width//4, 3*self.width//4):\n            sweep_freq = int(self.height//2 + (self.height//4) * (i - self.width//4) / (self.width//2))\n            if sweep_freq < self.height:\n                transition[sweep_freq, i] += 0.3\n        \n        # Normalize\n        transition = (transition - transition.mean()) / (transition.std() + 1e-6)\n        transition = np.clip(transition, -3, 3) / 3\n        \n        return transition\n    \n    def _generate_fallback_data(self):\n        \"\"\"Generate synthetic data as fallback\"\"\"\n        print(\"Using synthetic fallback data\")\n        # Simple synthetic spectrograms\n        source_a = torch.randn(self.height, self.width) * 0.5\n        source_b = torch.randn(self.height, self.width) * 0.5\n        noise = torch.randn(self.height, self.width) * 0.05\n        \n        # Simple crossfade target\n        target = torch.zeros_like(source_a)\n        for i in range(self.width):\n            alpha = i / self.width\n            target[:, i] = (1 - alpha) * source_a[:, i] + alpha * source_b[:, i]\n        \n        inputs = torch.stack([source_a, source_b, noise], dim=0)\n        target = target.unsqueeze(0)\n        \n        return inputs, target\n\n# Dataset configuration\nprint(\"Setting up audio dataset...\")\nprint(\"Instructions:\")\nprint(\"   1. Upload your audio files to Kaggle (Add Data ‚Üí Upload)\")\nprint(\"   2. Update AUDIO_DATA_PATH below to point to your uploaded folder\")\nprint(\"   3. Supported formats: WAV, MP3, FLAC, M4A\")\nprint(\"   4. Recommended: at least 100+ audio files for good training\")\n\n# Configure your audio data path here\nAUDIO_DATA_PATH = \"/kaggle/input/your-audio-dataset\"  # Update this path!\n\n# Check if audio data exists\nif not os.path.exists(AUDIO_DATA_PATH):\n    print(f\"Audio data path not found: {AUDIO_DATA_PATH}\")\n    print(\"Please update AUDIO_DATA_PATH to point to your uploaded audio files\")\n    print(\"Creating minimal synthetic dataset for demonstration...\")\n    \n    # Create a minimal synthetic dataset as fallback\n    class FallbackDataset(Dataset):\n        def __init__(self, num_samples=1000):\n            self.num_samples = num_samples\n            \n        def __len__(self):\n            return self.num_samples\n            \n        def __getitem__(self, idx):\n            # Generate simple synthetic data\n            source_a = torch.randn(SPECTROGRAM_HEIGHT, SPECTROGRAM_WIDTH) * 0.5\n            source_b = torch.randn(SPECTROGRAM_HEIGHT, SPECTROGRAM_WIDTH) * 0.5\n            noise = torch.randn(SPECTROGRAM_HEIGHT, SPECTROGRAM_WIDTH) * 0.05\n            \n            # Create crossfade target\n            target = torch.zeros_like(source_a)\n            for i in range(SPECTROGRAM_WIDTH):\n                alpha = i / SPECTROGRAM_WIDTH\n                target[:, i] = (1 - alpha) * source_a[:, i] + alpha * source_b[:, i]\n            \n            inputs = torch.stack([source_a, source_b, noise], dim=0)\n            target = target.unsqueeze(0)\n            \n            return inputs, target\n    \n    # Use fallback dataset\n    train_dataset = FallbackDataset(TRAIN_SAMPLES)\n    val_dataset = FallbackDataset(VAL_SAMPLES)\n    \n    print(f\"Using fallback synthetic dataset\")\n    print(f\"   Training samples: {len(train_dataset):,}\")\n    print(f\"   Validation samples: {len(val_dataset):,}\")\n    \nelse:\n    # Use real audio dataset\n    print(f\"Found audio data at: {AUDIO_DATA_PATH}\")\n    \n    try:\n        # Create datasets\n        train_dataset = AudioTransitionDataset(\n            audio_dir=AUDIO_DATA_PATH,\n            spectrogram_size=(SPECTROGRAM_HEIGHT, SPECTROGRAM_WIDTH),\n            sample_rate=SAMPLE_RATE,\n            hop_length=HOP_LENGTH,\n            split='train',\n            val_split=0.2\n        )\n        \n        val_dataset = AudioTransitionDataset(\n            audio_dir=AUDIO_DATA_PATH,\n            spectrogram_size=(SPECTROGRAM_HEIGHT, SPECTROGRAM_WIDTH),\n            sample_rate=SAMPLE_RATE,\n            hop_length=HOP_LENGTH,\n            split='val',\n            val_split=0.2\n        )\n        \n        print(f\"Real audio dataset created successfully!\")\n        \n    except Exception as e:\n        print(f\"Error creating audio dataset: {e}\")\n        print(\"Falling back to synthetic dataset...\")\n        \n        # Fallback to synthetic\n        train_dataset = FallbackDataset(TRAIN_SAMPLES)\n        val_dataset = FallbackDataset(VAL_SAMPLES)\n\n# Test dataset\ntest_inputs, test_targets = train_dataset[0]\nprint(f\"Dataset ready!\")\nprint(f\"   Training samples: {len(train_dataset):,}\")\nprint(f\"   Validation samples: {len(val_dataset):,}\")\nprint(f\"   Input shape: {test_inputs.shape}\")\nprint(f\"   Target shape: {test_targets.shape}\")\n\n# Create data loaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\nprint(f\"Data loaders created!\")\nprint(f\"   Training batches: {len(train_loader)}\")\nprint(f\"   Validation batches: {len(val_loader)}\")\n\ndel test_inputs, test_targets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e5a51cae","cell_type":"markdown","source":"### How to Upload Your Audio Data to Kaggle:\n\n1. **Prepare Your Audio Files:**\n   - Organize your audio files in folders (e.g., by genre, artist, etc.)\n   - Supported formats: WAV, MP3, FLAC, M4A\n   - Recommended: 100+ audio files for good training results\n   - Each file should be at least 15 seconds long\n\n2. **Upload to Kaggle:**\n   - Click \"Add Data\" ‚Üí \"Upload\" in the right panel\n   - Upload your audio folder as a ZIP file\n   - Wait for upload to complete\n   - Note the dataset path (usually `/kaggle/input/your-dataset-name/`)\n\n3. **Update the Path:**\n   - Modify the `AUDIO_DATA_PATH` variable below\n   - Run the dataset creation cell","metadata":{}},{"id":"37c47ba4","cell_type":"markdown","source":"## Initialize Training Components\n\nSet up the trainer class with model, optimizer, loss function, and data loaders for training.\n\n","metadata":{}},{"id":"4584acec","cell_type":"code","source":"# Initialize model\nmodel = ProductionUNet(\n    in_channels=IN_CHANNELS,\n    out_channels=OUT_CHANNELS,\n    model_dim=MODEL_DIM\n).to(device)\n\nprint(f\"Model initialized!\")\nprint(f\"Parameters: {model.count_parameters():,}\")\nprint(f\"Device: {device}\")\n\n# Initialize optimizer with learning rate scheduling\noptimizer = optim.AdamW(\n    model.parameters(),\n    lr=LEARNING_RATE,\n    weight_decay=1e-5,\n    betas=(0.9, 0.999)\n)\n\n# Learning rate scheduler with warmup\ndef get_lr_schedule_with_warmup(optimizer, warmup_epochs, total_epochs):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            # Warmup phase\n            return (epoch + 1) / warmup_epochs\n        else:\n            # Cosine annealing phase\n            progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n            return 0.5 * (1 + np.cos(np.pi * progress))\n    \n    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\nscheduler = get_lr_schedule_with_warmup(optimizer, WARMUP_EPOCHS, NUM_EPOCHS)\n\n# Loss function\ncriterion = nn.MSELoss()\n\n# Initialize tensorboard\nwriter = SummaryWriter('logs/kaggle_training')\n\nprint(\"Training components initialized!\")\nprint(f\"   Optimizer: AdamW\")\nprint(f\"   Learning Rate: {LEARNING_RATE}\")\nprint(f\"   Scheduler: Warmup + Cosine Annealing\")\nprint(f\"   Loss Function: MSE\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f866a744-9ff6-45ac-92f6-32d3bf004f69","cell_type":"markdown","source":"## Load Checkpoint (Optional)\n\nLoad a pre-trained checkpoint to continue training from a previous state. Update the checkpoint path below if you have one.","metadata":{}},{"id":"23ae1502-22f8-4439-a5ba-2d0f08e96754","cell_type":"code","source":"# Checkpoint loading configuration\nCHECKPOINT_PATH = '/kaggle/input/checkpoint5k/other/default/1/best_model_kaggle.pt'  # Set to checkpoint path to resume training\n# Example: CHECKPOINT_PATH = \"/kaggle/input/your-checkpoint/best_model_kaggle.pt\"\n# Or: CHECKPOINT_PATH = \"checkpoints/model_epoch_10.pt\"\n\n# Initialize training state variables\nstart_epoch = 0\nbest_val_loss = float('inf')\ntrain_losses = []\nval_losses = []\nlearning_rates = []\n\n# Load checkpoint if specified\nif CHECKPOINT_PATH and os.path.exists(CHECKPOINT_PATH):\n    print(f\"Loading checkpoint from: {CHECKPOINT_PATH}\")\n    \n    try:\n        checkpoint = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=False)\n        \n        # Load model state\n        model.load_state_dict(checkpoint['model_state_dict'])\n        print(f\"Model state loaded successfully\")\n        \n        # Load optimizer state\n        if 'optimizer_state_dict' in checkpoint:\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            print(f\"Optimizer state loaded successfully\")\n        \n        # Load scheduler state\n        if 'scheduler_state_dict' in checkpoint:\n            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n            print(f\"Scheduler state loaded successfully\")\n        \n        # Load training state\n        if 'epoch' in checkpoint:\n            start_epoch = checkpoint['epoch']\n            print(f\"Resuming from epoch {start_epoch}\")\n        \n        if 'best_val_loss' in checkpoint:\n            best_val_loss = checkpoint['best_val_loss']\n            print(f\"Best validation loss: {best_val_loss:.6f}\")\n        \n        # Load training history if available\n        if 'train_losses' in checkpoint:\n            train_losses = checkpoint.get('train_losses', [])\n            val_losses = checkpoint.get('val_losses', [])\n            learning_rates = checkpoint.get('learning_rates', [])\n            print(f\"Training history loaded ({len(train_losses)} epochs)\")\n        \n        # Verify model configuration matches\n        if 'model_config' in checkpoint:\n            config = checkpoint['model_config']\n            if (config.get('in_channels') == IN_CHANNELS and \n                config.get('out_channels') == OUT_CHANNELS and \n                config.get('model_dim') == MODEL_DIM):\n                print(f\"Model configuration verified\")\n            else:\n                print(f\" Model configuration mismatch:\")\n                print(f\"   Checkpoint: {config}\")\n                print(f\"   Current: in_channels={IN_CHANNELS}, out_channels={OUT_CHANNELS}, model_dim={MODEL_DIM}\")\n        \n        print(f\" Checkpoint loaded successfully!\")\n        \n    except Exception as e:\n        print(f\" Error loading checkpoint: {e}\")\n        print(f\"   Continuing with fresh training...\")\n        start_epoch = 0\n        best_val_loss = float('inf')\n        train_losses = []\n        val_losses = []\n        learning_rates = []\n\nelif CHECKPOINT_PATH:\n    print(f\"  Checkpoint path specified but file not found: {CHECKPOINT_PATH}\")\n    print(f\"   Starting fresh training...\")\n    \nelse:\n    print(f\" No checkpoint specified - starting fresh training\")\n\nprint(f\"  Training configuration:\")\nprint(f\"  Starting epoch: {start_epoch + 1}\")\nprint(f\"  Target epochs: {NUM_EPOCHS}\")\nprint(f\"  Best validation loss: {best_val_loss:.6f}\" if best_val_loss != float('inf') else \"   Best validation loss: Not set\")\nprint(f\"  Training history: {len(train_losses)} previous epochs\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2ab62d50","cell_type":"markdown","source":"## Training Loop Implementation\n\nExecute the main training loop with epoch-by-epoch training, validation, and progress monitoring.","metadata":{}},{"id":"00574b9a","cell_type":"code","source":"print(\" Starting training...\")\nprint(f\" Epochs: {NUM_EPOCHS} | Batch size: {BATCH_SIZE}\")\nprint(\"=\" * 60)\n\nstart_time = time.time()\n\nfor epoch in range(start_epoch, NUM_EPOCHS):\n    current_epoch = epoch + 1\n    \n    print(f\"\\n Epoch {current_epoch}/{NUM_EPOCHS}\")\n    print(\"-\" * 50)\n    \n    # Training phase\n    model.train()\n    epoch_train_loss = 0.0\n    num_train_batches = len(train_loader)\n    \n    for batch_idx, (inputs, targets) in enumerate(train_loader):\n        inputs = inputs.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n        \n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        \n        # Backward pass\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        # Update weights\n        if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n        \n        epoch_train_loss += loss.item()\n        \n        # Progress logging\n        if batch_idx % 200 == 0:\n            progress = batch_idx / num_train_batches * 100\n            print(f\"   Batch {batch_idx}/{num_train_batches} ({progress:.1f}%) - Loss: {loss.item():.6f}\")\n    \n    avg_train_loss = epoch_train_loss / num_train_batches\n    train_losses.append(avg_train_loss)\n    \n    # Validation phase\n    model.eval()\n    epoch_val_loss = 0.0\n    num_val_batches = len(val_loader)\n    \n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs = inputs.to(device, non_blocking=True)\n            targets = targets.to(device, non_blocking=True)\n            \n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            epoch_val_loss += loss.item()\n    \n    avg_val_loss = epoch_val_loss / num_val_batches\n    val_losses.append(avg_val_loss)\n    \n    # Update learning rate\n    scheduler.step()\n    current_lr = optimizer.param_groups[0]['lr']\n    learning_rates.append(current_lr)\n    \n    # Log to tensorboard\n    writer.add_scalar('Loss/Train', avg_train_loss, current_epoch)\n    writer.add_scalar('Loss/Validation', avg_val_loss, current_epoch)\n    writer.add_scalar('Learning_Rate', current_lr, current_epoch)\n    \n    # Print epoch summary\n    print(f\" Train Loss: {avg_train_loss:.6f}\")\n    print(f\" Val Loss: {avg_val_loss:.6f}\")\n    print(f\" Learning Rate: {current_lr:.2e}\")\n    \n    # Save best model\n    is_best = avg_val_loss < best_val_loss\n    if is_best:\n        best_val_loss = avg_val_loss\n        # Save best model\n        best_checkpoint = {\n            'epoch': current_epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'train_loss': avg_train_loss,\n            'val_loss': avg_val_loss,\n            'best_val_loss': best_val_loss,\n            'model_config': {\n            'in_channels': IN_CHANNELS,\n            'out_channels': OUT_CHANNELS,\n            'model_dim': MODEL_DIM\n            }\n        }\n        torch.save(best_checkpoint, 'checkpoints/best_model_kaggle.pt')\n        print(f\" New best model saved! (Val Loss: {best_val_loss:.6f})\")\n        \n        # Save regular checkpoint every 5 epochs\n        if current_epoch % 5 == 0:\n            checkpoint = {\n                'epoch': current_epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'train_loss': avg_train_loss,\n                'val_loss': avg_val_loss,\n                'best_val_loss': best_val_loss,\n                'model_config': {\n                'in_channels': IN_CHANNELS,\n                'out_channels': OUT_CHANNELS,\n                'model_dim': MODEL_DIM\n                }\n            }\n            torch.save(checkpoint, f'checkpoints/model_epoch_{current_epoch}.pt')\n            print(f\"üíæ Checkpoint saved: model_epoch_{current_epoch}.pt\")\n            \n            # Memory cleanup\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                gc.collect()\n\n    # Training completed\n    total_time = time.time() - start_time\n    print(f\"\\n Training completed!\")\n    print(f\" Total time: {total_time:.1f}s ({total_time/60:.1f}m)\")\n    print(f\" Best validation loss: {best_val_loss:.6f}\")\n\n    # Save final checkpoint\n    final_checkpoint = {\n        'epoch': NUM_EPOCHS,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'learning_rates': learning_rates,\n        'best_val_loss': best_val_loss,\n        'total_training_time': total_time,\n        'model_config': {\n        'in_channels': IN_CHANNELS,\n        'out_channels': OUT_CHANNELS,\n        'model_dim': MODEL_DIM\n        }\n    }\n    torch.save(final_checkpoint, 'checkpoints/final_model_kaggle.pt')\n    print(\" Final checkpoint saved: final_model_kaggle.pt\")\n\n    # Close tensorboard writer\n    writer.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}