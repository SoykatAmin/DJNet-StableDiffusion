# DJNet Kaggle Training Configuration

# Model Configuration
model:
  pretrained_model: "runwayml/stable-diffusion-v1-5"
  freeze_encoder: false
  in_channels: 3
  out_channels: 1

# Data Configuration - Optimized for Kaggle
data:
  data_dir: "./data"
  json_files: null
  sample_rate: 16000
  n_fft: 1024
  hop_length: 256
  n_mels: 128
  spectrogram_size: [128, 128]
  normalize: true
  augment: false
  cache_spectrograms: true
  train_split: 0.8
  val_split: 0.2

# Training Configuration - Optimized for Kaggle GPU
training:
  batch_size: 4  # Reduced for GPU memory constraints
  val_batch_size: 4
  num_epochs: 20  # More epochs for better results
  learning_rate: 1e-4
  weight_decay: 1e-2
  min_lr: 1e-6
  use_scheduler: true
  
  # Memory optimization
  mixed_precision: true
  gradient_accumulation_steps: 2  # Effective batch size = 8
  max_grad_norm: 1.0
  
  # Frequent saving for Kaggle
  log_every: 25
  save_every: 100  # Save more frequently
  validate_every: 50
  save_dir: "./checkpoints"
  
  # Hardware - Kaggle specific
  device: "auto"
  num_workers: 2  # Reduced for Kaggle
  pin_memory: true

# Diffusion Configuration
diffusion:
  num_train_timesteps: 1000
  beta_start: 0.0001
  beta_end: 0.02
  beta_schedule: "linear"
  clip_sample: true

# Loss Configuration
loss:
  mse_weight: 1.0
  perceptual_weight: 0.1
  temporal_weight: 0.05

# Weights & Biases Configuration
wandb:
  project: "djnet-kaggle"
  run_name: null
  tags: ["djnet", "diffusion", "audio", "kaggle"]
  notes: "Kaggle training run for DJ transition generation"

# Inference Configuration
inference:
  num_inference_steps: 25  # Faster inference for testing
  guidance_scale: 1.0
  
# Resume Configuration
resume:
  checkpoint_path: null
  
# Evaluation Configuration
evaluation:
  generate_samples: true
  num_samples: 4  # Fewer samples for memory
  save_samples: true
  sample_inference_steps: 10  # Fast sampling for evaluation
